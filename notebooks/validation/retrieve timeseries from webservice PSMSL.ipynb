{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation DataFrame to be used for Analysis high-frequency tidal gauge data (RWS) and comparison with monthly metric averages (PSMSL website).\n",
    "\n",
    "This online notebook retrieves data from the PSMSL website and transforms the data in such format it can be used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import functools\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the url from where the PSMSL monthly metric data is obtained. This is the data as provided by RWS to PSMSL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {\n",
    "    'metric_monthly': 'http://www.psmsl.org/data/obtaining/met.monthly.data/met_monthly.zip',\n",
    "}\n",
    "dataset_name = 'metric_monthly'\n",
    "dataset_name_compact = 'met_monthly'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following 6 main tidal gauge stations are selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names of the stations of interest\n",
    "main_stations = {\n",
    "    20: {\n",
    "        'name': 'Vlissingen'\n",
    "    },\n",
    "    22: {\n",
    "        'name': 'Hoek van Holland'\n",
    "    },\n",
    "    23: {\n",
    "        'name': 'Den Helder'\n",
    "    },\n",
    "    24: {\n",
    "        'name': 'Delfzijl'\n",
    "    },\n",
    "    25: {\n",
    "        'name': 'Harlingen'\n",
    "    },\n",
    "    32: {\n",
    "        'name': 'IJmuiden'\n",
    "    }\n",
    "}\n",
    "# the main stations are defined by their ids\n",
    "main_stations_idx = list(main_stations.keys())\n",
    "# main_stations_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the zipfiles from the PSMSL website, enter the zipfile and extract the selected stations from the soure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the zipfile\n",
    "resp = requests.get(urls[dataset_name])\n",
    "\n",
    "# we can read the zipfile\n",
    "stream = io.BytesIO(resp.content)\n",
    "zf = zipfile.ZipFile(stream)\n",
    "\n",
    "# this list contains a table of\n",
    "# station ID, latitude, longitude, station name,\n",
    "# coastline code, station code, and quality flag\n",
    "csvtext = zf.read('{}/filelist.txt'.format(dataset_name_compact))\n",
    "\n",
    "stations = pd.read_csv(\n",
    "    io.BytesIO(csvtext),\n",
    "    sep=';',\n",
    "    names=('id', 'lat', 'lon', 'name', 'coastline_code',\n",
    "           'station_code', 'quality'),\n",
    "    converters={\n",
    "        'name': str.strip,\n",
    "        'quality': str.strip\n",
    "    }\n",
    ")\n",
    "stations = stations.set_index('id')\n",
    "\n",
    "# the dutch stations in the PSMSL database, make a copy\n",
    "# or use stations.coastline_code == 150 for all dutch stations\n",
    "selected_stations = stations.loc[main_stations_idx].copy()\n",
    "# set the main stations, this should be a list of 6 stations\n",
    "# selected_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each station has a number of files that you can look at.\n",
    "# here we define a template for each filename\n",
    "\n",
    "# stations that we are using for our computation\n",
    "# define the name formats for the relevant files\n",
    "names = {\n",
    "    'url': 'http://www.psmsl.org/data/obtaining/rlr.diagrams/{id}.php',\n",
    "    'data': '{dataset}/data/{id}.metdata'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need some functions to get the correct url and check for missing values and to retrieve the data from the zipfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(station, dataset):\n",
    "    \"\"\"return the url of the station information (diagram and datum)\"\"\"\n",
    "    info = dict(\n",
    "        dataset=dataset,\n",
    "        id=station.name\n",
    "    )\n",
    "    url = names['url'].format(**info)\n",
    "    return url\n",
    "\n",
    "\n",
    "# fill in the dataset parameter using the global dataset_name\n",
    "f = functools.partial(get_url, dataset=dataset_name)\n",
    "# compute the url for each station\n",
    "selected_stations['url'] = selected_stations.apply(f, axis=1)\n",
    "# selected_stations\n",
    "\n",
    "\n",
    "def missing2nan(value, missing=-99999):\n",
    "    \"\"\"\n",
    "    convert the value to nan if the float of value equals the missing value\n",
    "    \"\"\"\n",
    "    value = float(value)\n",
    "    if value == missing:\n",
    "        return np.nan\n",
    "    return value\n",
    "\n",
    "\n",
    "def get_data(station, dataset):\n",
    "    \"\"\"get data for the station (pandas record) from the dataset (url)\"\"\"\n",
    "    info = dict(\n",
    "        dataset=dataset,\n",
    "        id=station.name\n",
    "    )\n",
    "    bytes = zf.read(names['data'].format(**info))\n",
    "    df = pd.read_csv(\n",
    "        io.BytesIO(bytes),\n",
    "        sep=';',\n",
    "        names=('year', 'height', 'interpolated', 'flags')\n",
    "    )\n",
    "    df['station'] = station.name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PSMSL data is stored in decimalyear, create a function, so it is transformed into python datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertdecimalyear2datetime(x_months):\n",
    "    x_month_datetime = []\n",
    "    for month in x_months:\n",
    "        dmonth, year = math.modf(month)\n",
    "        x_month_datetime.append(pd.datetime(int(year),\n",
    "                                            int(np.ceil(dmonth*12)), 15))\n",
    "    x_months_series = pd.Series(x_month_datetime)\n",
    "    return x_months_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the helper functions are in place retrieve the data and store in the selected stations dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for all stations\n",
    "f = functools.partial(get_data, dataset=dataset_name_compact)\n",
    "# look up the data for each station\n",
    "selected_stations['data'] = [f(station) for _, station in\n",
    "                             selected_stations.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now have data for each station\n",
    "#selected_stations[['name', 'data']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create concatenated dataframe of all locations with all waterlevels in cm+NAP for all months recorded at PSMSL and return as dataframe accessible in 'paired difference analysis RWS and PSMSL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_psmsl = pd.DataFrame()\n",
    "for station in selected_stations.iterrows():\n",
    "    loc_name = station[1]['name']\n",
    "    x_months = station[1]['data']['year']\n",
    "    y_height = station[1]['data']['height']\n",
    "\n",
    "    # The daily, monthly and annual heights are expressed in millimetres.\n",
    "    # The dates are in decimal years (centred on the 15th day of\n",
    "    # the month for monthly values and at midday for daily values).\n",
    "    y_height /= 10  # to get centimeters\n",
    "    x_months = convertdecimalyear2datetime(x_months)\n",
    "\n",
    "    df_loc = pd.DataFrame(data=y_height.as_matrix(),\n",
    "                          index=x_months,\n",
    "                          columns=[loc_name+'_'+'WATHTE_cmNAP'])\n",
    "    df_psmsl = pd.concat((df_psmsl, df_loc), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# df_psmsl.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
