{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Functions taken from the Dutch Monitor notebook to get a DataFrame which can be used for the PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this is a list of packages that are used in this notebook\n",
    "# these come with python\n",
    "import io\n",
    "import zipfile\n",
    "import functools\n",
    "import bisect\n",
    "import datetime\n",
    "\n",
    "\n",
    "# you can install these packages using pip or anaconda\n",
    "# (requests numpy pandas bokeh pyproj statsmodels)\n",
    "\n",
    "# for downloading\n",
    "import requests\n",
    "import netCDF4\n",
    "\n",
    "# computation libraries\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "\n",
    "# coordinate systems\n",
    "import pyproj \n",
    "\n",
    "# statistics\n",
    "import statsmodels.api as sm\n",
    "import statsmodels\n",
    "\n",
    "# plotting\n",
    "import bokeh.charts\n",
    "import bokeh.io\n",
    "import bokeh.plotting\n",
    "import bokeh.tile_providers\n",
    "import bokeh.palettes\n",
    "\n",
    "import windrose\n",
    "import matplotlib.colors\n",
    "import matplotlib.cm\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.projections.register_projection(windrose.WindroseAxes)\n",
    "# print(matplotlib.projections.get_projection_names())\n",
    "import cmocean.cm\n",
    "\n",
    "# displaying things\n",
    "from ipywidgets import Image\n",
    "import IPython.display\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Sea-level explained  \n",
    "=======\n",
    "The sea-level is dependent on several factors. We call these factors explanatory, exogenuous or independent variables. The main factors that influence the monthly and annual sea level include wind, pressure, river discharge, tide and oscilations in the ocean. Based on previous analysis we include wind and nodal tide as independent variables. To be able to include wind, we use the monthly 10m wind based on the NCEP reanlysis of the NCAR. To be more specific we include the squared u and v wind components. Unfortunately the wind series only go back to 1948. To be able to include them without having to discard the sea level measurements before 1948, we fill in the missing data with the mean. \n",
    "\n",
    "We don't include timeseries of volume based explanatory variables like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def find_closest(lat, lon, lat_i, lon_i):\n",
    "    \"\"\"lookup the index of the closest lat/lon\"\"\"\n",
    "    Lon, Lat = np.meshgrid(lon, lat)\n",
    "    idx = np.argmin(((Lat - lat_i)**2 + (Lon - lon_i)**2))\n",
    "    Lat.ravel()[idx], Lon.ravel()[idx]\n",
    "    [i, j] = np.unravel_index(idx, Lat.shape)\n",
    "    return i, j\n",
    "\n",
    "def make_wind_df(lat_i, lon_i):\n",
    "    \"\"\"create a dataset for wind, for 1 latitude/longitude\"\"\"\n",
    "    u_file = 'http://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/ncep.reanalysis.derived/surface_gauss/uwnd.10m.mon.mean.nc'\n",
    "    v_file = 'http://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/ncep.reanalysis.derived/surface_gauss/vwnd.10m.mon.mean.nc'\n",
    "\n",
    "    # open the 2 files\n",
    "    ds_u = netCDF4.Dataset(u_file)\n",
    "    ds_v = netCDF4.Dataset(v_file)\n",
    "    # read lat,lon, time from 1 dataset\n",
    "    lat, lon, time = ds_u.variables['lat'][:], ds_u.variables['lon'][:], ds_u.variables['time'][:]\n",
    "    # check with the others\n",
    "    lat_v, lon_v, time_v = ds_v.variables['lat'][:], ds_v.variables['lon'][:], ds_v.variables['time'][:]\n",
    "    assert (lat == lat_v).all() and (lon == lon_v).all() and (time == time_v).all()\n",
    "    # convert to datetime\n",
    "    t = netCDF4.num2date(time, ds_u.variables['time'].units)\n",
    "    \n",
    "    # MVH-01082017 : moved function find_closest outside this function.\n",
    "    \n",
    "    # this is the index where we want our data\n",
    "    i, j = find_closest(lat, lon, lat_i, lon_i)\n",
    "    # get the u, v variables\n",
    "    #print('found point', lat[i], lon[j])\n",
    "    u = ds_u.variables['uwnd'][:, i, j]\n",
    "    v = ds_v.variables['vwnd'][:, i, j]\n",
    "    # compute derived quantities\n",
    "    speed = np.sqrt(u ** 2 + v **2)\n",
    "    # compute direction in 0-2pi domain\n",
    "    direction = np.mod(np.angle(u + v * 1j), 2*np.pi)\n",
    "    # put everything in a dataframe\n",
    "    wind_df = pandas.DataFrame(data=dict(u=u, v=v, t=t, speed=speed, direction=direction))\n",
    "    # return it\n",
    "    return wind_df\n",
    "\n",
    "lat_i=53\n",
    "lon_i=3\n",
    "wind_df = make_wind_df(lat_i=lat_i, lon_i=lon_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "http://www.psmsl.org/data/obtaining/rlr.php:\n",
    "\n",
    "\"In the past, the PSMSL also included the the Netherlands data in the above category of Metric records acceptable for time series work. These records are expressed relative to the national level system Normaal Amsterdamsch Peil (NAP). However, a recent re-levelling of NAP in 2005 introduced a small datum shift for the tide gauge time series. In order to maintain utility of these long records, we have reclassified most of the Netherlands records as RLR and introduced different RLR factors for the periods before and after 2005. While these records do not meet the strict definition of RLR and may still include prior re-levelling adjustments, we believe this represents the best path forward.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "urls = {\n",
    "    'metric_monthly': 'http://www.psmsl.org/data/obtaining/met.monthly.data/met_monthly.zip',\n",
    "    'rlr_monthly': 'http://www.psmsl.org/data/obtaining/rlr.monthly.data/rlr_monthly.zip',\n",
    "    'rlr_annual': 'http://www.psmsl.org/data/obtaining/rlr.annual.data/rlr_annual.zip'\n",
    "}\n",
    "dataset_name = 'rlr_monthly'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the website above the RLR Diagrom for Vlissingen is shown. It shows that the MSL (2007) level is 6.976 meters above RLR (2007), where the NAP 2005 - onwards level is 0.046m below the MSL (2007) level. This explains the `'rlr2nap': lambda x: x - (6976-46)` for Vlissingen in the code block below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# these compute the rlr back to NAP (ignoring the undoing of the NAP correction)\n",
    "main_stations = {\n",
    "    20: {\n",
    "        'name': 'Vlissingen', \n",
    "        'rlr2nap': lambda x: x - (6976-46)\n",
    "    },\n",
    "    22: {\n",
    "        'name': 'Hoek van Holland', \n",
    "        'rlr2nap': lambda x:x - (6994 - 121)\n",
    "    },\n",
    "    23: {\n",
    "        'name': 'Den Helder', \n",
    "        'rlr2nap': lambda x: x - (6988-42)\n",
    "    },\n",
    "    24: {\n",
    "        'name': 'Delfzijl', \n",
    "        'rlr2nap': lambda x: x - (6978-155)\n",
    "    },\n",
    "    25: {\n",
    "        'name': 'Harlingen', \n",
    "        'rlr2nap': lambda x: x - (7036-122)\n",
    "    },\n",
    "    32: {\n",
    "        'name': 'IJmuiden', \n",
    "        'rlr2nap': lambda x: x - (7033-83)\n",
    "    },\n",
    "#     1551: {\n",
    "#         'name': 'Roompot buiten',\n",
    "#         'rlr2nap': lambda x: x - (7011-17)\n",
    "#     },\n",
    "#     9: {\n",
    "#         'name': 'Maassluis',\n",
    "#         'rlr2nap': lambda x: x - (6983-184)\n",
    "#     },\n",
    "#     236: {\n",
    "#         'name': 'West-Terschelling',\n",
    "#         'rlr2nap': lambda x: x - (7011-54)\n",
    "#     }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# the main stations are defined by their ids\n",
    "main_stations_idx = list(main_stations.keys())\n",
    "#main_stations_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# download the zipfile\n",
    "resp = requests.get(urls[dataset_name])\n",
    "\n",
    "# we can read the zipfile\n",
    "stream = io.BytesIO(resp.content)\n",
    "zf = zipfile.ZipFile(stream)\n",
    "\n",
    "# this list contains a table of \n",
    "# station ID, latitude, longitude, station name, coastline code, station code, and quality flag\n",
    "csvtext = zf.read('{}/filelist.txt'.format(dataset_name))\n",
    "\n",
    "stations = pandas.read_csv(\n",
    "    io.BytesIO(csvtext), \n",
    "    sep=';',\n",
    "    names=('id', 'lat', 'lon', 'name', 'coastline_code', 'station_code', 'quality'),\n",
    "    converters={\n",
    "        'name': str.strip,\n",
    "        'quality': str.strip\n",
    "    }\n",
    ")\n",
    "stations = stations.set_index('id')\n",
    "\n",
    "# the dutch stations in the PSMSL database, make a copy\n",
    "# or use stations.coastline_code == 150 for all dutch stations\n",
    "selected_stations = stations.ix[main_stations_idx].copy()\n",
    "# set the main stations, this should be a list of 6 stations\n",
    "#selected_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we have defined which tide gauges we are monitoring we can start downloading the relevant data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# each station has a number of files that you can look at.\n",
    "# here we define a template for each filename\n",
    "\n",
    "# stations that we are using for our computation\n",
    "# define the name formats for the relevant files\n",
    "names = {\n",
    "    'datum': '{dataset}/RLR_info/{id}.txt',\n",
    "    'diagram': '{dataset}/RLR_info/{id}.png',\n",
    "    'url': 'http://www.psmsl.org/data/obtaining/rlr.diagrams/{id}.php',\n",
    "    'data': '{dataset}/data/{id}.rlrdata',\n",
    "    'doc': '{dataset}/docu/{id}.txt',\n",
    "    'contact': '{dataset}/docu/{id}_auth.txt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_url(station, dataset):\n",
    "    \"\"\"return the url of the station information (diagram and datum)\"\"\"\n",
    "    info = dict(\n",
    "        dataset=dataset,\n",
    "        id=station.name\n",
    "    )\n",
    "    url = names['url'].format(**info)\n",
    "    return url\n",
    "# fill in the dataset parameter using the global dataset_name\n",
    "f = functools.partial(get_url, dataset=dataset_name)\n",
    "# compute the url for each station\n",
    "selected_stations['url'] = selected_stations.apply(f, axis=1)\n",
    "#selected_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def missing2nan(value, missing=-99999):\n",
    "    \"\"\"convert the value to nan if the float of value equals the missing value\"\"\"\n",
    "    value = float(value)\n",
    "    if value == missing:\n",
    "        return np.nan\n",
    "    return value\n",
    "\n",
    "def year2date(year_fraction, dtype):\n",
    "    startpoints = np.linspace(0, 1, num=12, endpoint=False)\n",
    "    remainder = np.mod(year_fraction, 1)\n",
    "    year = np.floor_divide(year_fraction, 1).astype('int')\n",
    "    month = np.searchsorted(startpoints, remainder)\n",
    "    dates = [\n",
    "        datetime.datetime(year_i, month_i, 1) \n",
    "        for year_i, month_i \n",
    "        in zip(year, month)\n",
    "    ]\n",
    "    datetime64s = np.asarray(dates, dtype=dtype)\n",
    "    return datetime64s\n",
    "\n",
    "def get_data(station, dataset):\n",
    "    \"\"\"get data for the station (pandas record) from the dataset (url)\"\"\"\n",
    "    info = dict(\n",
    "        dataset=dataset,\n",
    "        id=station.name\n",
    "    )    \n",
    "    bytes = zf.read(names['data'].format(**info))\n",
    "    df = pandas.read_csv(\n",
    "        io.BytesIO(bytes), \n",
    "        sep=';', \n",
    "        names=('year', 'height', 'interpolated', 'flags'),\n",
    "        converters={\n",
    "            \"height\": lambda x: main_stations[station.name]['rlr2nap'](missing2nan(x)),\n",
    "            \"interpolated\": str.strip,\n",
    "        }\n",
    "    )\n",
    "    df['station'] = station.name\n",
    "    df['t'] = year2date(df.year, dtype=wind_df.t.dtype)\n",
    "    # merge the wind and water levels\n",
    "    merged = pandas.merge(df, wind_df, how='left', on='t')\n",
    "    merged['u2'] = np.where(np.isnan(merged['u']), np.nanmean(merged['u']**2), merged['u']**2)\n",
    "    merged['v2'] = np.where(np.isnan(merged['v']), np.nanmean(merged['v']**2), merged['v']**2)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get data for all stations\n",
    "f = functools.partial(get_data, dataset=dataset_name)\n",
    "# look up the data for each station\n",
    "selected_stations['data'] = [f(station) for _, station in selected_stations.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "names = []\n",
    "for id, station in selected_stations.iterrows():\n",
    "    df = station['data'].ix[station['data'].year >= 1930]\n",
    "    dfs.append(df.set_index('year')['height'])\n",
    "    names.append(station['name'])\n",
    "merged = pandas.concat(dfs, axis=1)\n",
    "merged.columns = names\n",
    "diffs = merged.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = merged.copy()#merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the statistical model\n",
    "def linear_model(df, with_wind=True, with_season=True):\n",
    "    y = df['height']\n",
    "    X = np.c_[\n",
    "        df['year']-1970, \n",
    "        np.cos(2*np.pi*(df['year']-1970)/18.613),\n",
    "        np.sin(2*np.pi*(df['year']-1970)/18.613)\n",
    "    ]\n",
    "    month = np.mod(df['year'], 1) * 12.0\n",
    "    names = ['Constant', 'Trend', 'Nodal U', 'Nodal V']\n",
    "    if with_wind:\n",
    "        X = np.c_[\n",
    "            X, \n",
    "            df['u2'],\n",
    "            df['v2']\n",
    "        ]\n",
    "        names.extend(['Wind U^2', 'Wind V^2'])\n",
    "    if with_season:\n",
    "        for i in range(11):\n",
    "            X = np.c_[\n",
    "                X,\n",
    "                np.logical_and(month >= i, month < i+1)\n",
    "            ]\n",
    "            names.append('month_%s' % (i+1, ))\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X, missing='drop')\n",
    "    fit = model.fit()\n",
    "    return fit, names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
