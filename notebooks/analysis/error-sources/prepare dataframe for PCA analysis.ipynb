{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation DataFrame to be used for Principal Component Analysis applied to tidal gauge data along the Dutch coast "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook contains multiple functions and code cells responsible for retrieving data from PSMSL to be used for the PCA analysis. The core functions are taken from the Dutch Monitor notebook. The result of this notebook is used in the notebook 'application of PCA analysis'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a list of packages that are used in this notebook\n",
    "# these come with python\n",
    "import io\n",
    "import zipfile\n",
    "import functools\n",
    "import bisect\n",
    "import datetime\n",
    "\n",
    "# you can install these packages using pip or anaconda\n",
    "# (requests numpy pandas bokeh pyproj statsmodels)\n",
    "\n",
    "# for downloading\n",
    "import requests\n",
    "import netCDF4\n",
    "\n",
    "# computation libraries\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "# coordinate systems\n",
    "import pyproj\n",
    "\n",
    "# statistics\n",
    "import statsmodels.api as sm\n",
    "import statsmodels\n",
    "\n",
    "# plotting\n",
    "import bokeh.charts\n",
    "import bokeh.io\n",
    "import bokeh.plotting\n",
    "import bokeh.tile_providers\n",
    "import bokeh.palettes\n",
    "\n",
    "import windrose\n",
    "import matplotlib.colors\n",
    "import matplotlib.cm\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.projections.register_projection(windrose.WindroseAxes)\n",
    "# print(matplotlib.projections.get_projection_names())\n",
    "import cmocean.cm\n",
    "\n",
    "# displaying things\n",
    "from ipywidgets import Image\n",
    "import IPython.display\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea-level explained  \n",
    "=======\n",
    "The sea-level is dependent on several factors. We call these factors explanatory, exogeneous or independent variables. The main factors that influence the monthly and annual sea level include wind, pressure, river discharge, tide and oscilations in the ocean. Based on previous analysis we include wind and nodal tide as independent variables. To be able to include wind, we use the monthly 10m wind based on the NCEP reanlysis of the NCAR. To be more specific we include the squared u and v wind components. Unfortunately the wind series only go back to 1948. To be able to include them without having to discard the sea level measurements before 1948, we fill in the missing data with the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(lat, lon, lat_i, lon_i):\n",
    "    \"\"\"lookup the index of the closest lat/lon\"\"\"\n",
    "    Lon, Lat = np.meshgrid(lon, lat)\n",
    "    idx = np.argmin(((Lat - lat_i)**2 + (Lon - lon_i)**2))\n",
    "    Lat.ravel()[idx], Lon.ravel()[idx]\n",
    "    [i, j] = np.unravel_index(idx, Lat.shape)\n",
    "    return i, j\n",
    "\n",
    "\n",
    "def make_wind_df(lat_i, lon_i):\n",
    "    \"\"\"create a dataset for wind, for 1 latitude/longitude\"\"\"\n",
    "    u_file = 'http://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/ncep.reanalysis.derived/surface_gauss/uwnd.10m.mon.mean.nc'\n",
    "    v_file = 'http://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/ncep.reanalysis.derived/surface_gauss/vwnd.10m.mon.mean.nc'\n",
    "\n",
    "    # open the 2 files\n",
    "    ds_u = netCDF4.Dataset(u_file)\n",
    "    ds_v = netCDF4.Dataset(v_file)\n",
    "\n",
    "    # read lat,lon, time from 1 dataset\n",
    "    lat = ds_u.variables['lat'][:]\n",
    "    lon = ds_u.variables['lon'][:]\n",
    "    time = ds_u.variables['time'][:]\n",
    "    # check with the others\n",
    "    lat_v = ds_v.variables['lat'][:]\n",
    "    lon_v = ds_v.variables['lon'][:]\n",
    "    time_v = ds_v.variables['time'][:]\n",
    "    assert (lat == lat_v).all() and (lon == lon_v).all() and (time == time_v).all()\n",
    "    # convert to datetime\n",
    "    t = netCDF4.num2date(time, ds_u.variables['time'].units)\n",
    "\n",
    "    # this is the index where we want our data\n",
    "    i, j = find_closest(lat, lon, lat_i, lon_i)\n",
    "    # get the u, v variables\n",
    "    # print('found point', lat[i], lon[j])\n",
    "    u = ds_u.variables['uwnd'][:, i, j]\n",
    "    v = ds_v.variables['vwnd'][:, i, j]\n",
    "    # compute derived quantities\n",
    "    speed = np.sqrt(u ** 2 + v ** 2)\n",
    "    # compute direction in 0-2pi domain\n",
    "    direction = np.mod(np.angle(u + v * 1j), 2 * np.pi)\n",
    "    # put everything in a dataframe\n",
    "    wind_df = pandas.DataFrame(data=dict(u=u, v=v, t=t,\n",
    "                                         speed=speed, direction=direction))\n",
    "    # return it\n",
    "    return wind_df\n",
    "\n",
    "\n",
    "lat_i = 53\n",
    "lon_i = 3\n",
    "wind_df = make_wind_df(lat_i=lat_i, lon_i=lon_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PSMSL data is derived from the PSMSL website, where the data is described as follow for the Netherlands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In the past, the PSMSL also included the the Netherlands data in the above category of Metric records acceptable for time series work. These records are expressed relative to the national level system Normaal Amsterdamsch Peil (NAP). However, a recent re-levelling of NAP in 2005 introduced a small datum shift for the tide gauge time series. In order to maintain utility of these long records, we have reclassified most of the Netherlands records as RLR and introduced different RLR factors for the periods before and after 2005. While these records do not meet the strict definition of RLR and may still include prior re-levelling adjustments, we believe this represents the best path forward.\" [http://www.psmsl.org/data/obtaining/rlr.php]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {\n",
    "    'metric_monthly':\n",
    "    'http://www.psmsl.org/data/obtaining/met.monthly.data/met_monthly.zip',\n",
    "    'rlr_monthly':\n",
    "    'http://www.psmsl.org/data/obtaining/rlr.monthly.data/rlr_monthly.zip',\n",
    "    'rlr_annual':\n",
    "    'http://www.psmsl.org/data/obtaining/rlr.annual.data/rlr_annual.zip'\n",
    "}\n",
    "dataset_name = 'rlr_monthly'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the website above the RLR Diagrom for Vlissingen is shown. It shows that the MSL (2007) level is 6.976 meters above RLR (2007), where the NAP 2005 - onwards level is 0.046m below the MSL (2007) level. This explains the `'rlr2nap': lambda x: x - (6976-46)` for Vlissingen in the code block below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these compute the rlr back to NAP\n",
    "# lambda functions are not recommended by PEP8, \n",
    "# but not sure how to replace them\n",
    "main_stations = {\n",
    "    20: {\n",
    "        'name': 'Vlissingen',\n",
    "        'rlr2nap': lambda x: x - (6976-46)\n",
    "    },\n",
    "    22: {\n",
    "        'name': 'Hoek van Holland',\n",
    "        'rlr2nap': lambda x: x - (6994 - 121)\n",
    "    },\n",
    "    23: {\n",
    "        'name': 'Den Helder',\n",
    "        'rlr2nap': lambda x: x - (6988-42)\n",
    "    },\n",
    "    24: {\n",
    "        'name': 'Delfzijl',\n",
    "        'rlr2nap': lambda x: x - (6978-155)\n",
    "    },\n",
    "    25: {\n",
    "        'name': 'Harlingen',\n",
    "        'rlr2nap': lambda x: x - (7036-122)\n",
    "    },\n",
    "    32: {\n",
    "        'name': 'IJmuiden',\n",
    "        'rlr2nap': lambda x: x - (7033-83)\n",
    "    },\n",
    "#     1551: {\n",
    "#         'name': 'Roompot buiten',\n",
    "#         'rlr2nap': lambda x: x - (7011-17)\n",
    "#     },\n",
    "#     9: {\n",
    "#         'name': 'Maassluis',\n",
    "#         'rlr2nap': lambda x: x - (6983-184)\n",
    "#     },\n",
    "#     236: {\n",
    "#         'name': 'West-Terschelling',\n",
    "#         'rlr2nap': lambda x: x - (7011-54)\n",
    "#     }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main stations are defined by their ids\n",
    "main_stations_idx = list(main_stations.keys())\n",
    "# main_stations_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the zipfile\n",
    "resp = requests.get(urls[dataset_name])\n",
    "\n",
    "# we can read the zipfile\n",
    "stream = io.BytesIO(resp.content)\n",
    "zf = zipfile.ZipFile(stream)\n",
    "\n",
    "# this list contains a table of\n",
    "# station ID, latitude, longitude, station name,\n",
    "# coastline code, station code, and quality flag\n",
    "csvtext = zf.read('{}/filelist.txt'.format(dataset_name))\n",
    "\n",
    "stations = pandas.read_csv(\n",
    "    io.BytesIO(csvtext),\n",
    "    sep=';',\n",
    "    names=('id', 'lat', 'lon', 'name',\n",
    "           'coastline_code', 'station_code', 'quality'),\n",
    "    converters={\n",
    "        'name': str.strip,\n",
    "        'quality': str.strip\n",
    "    }\n",
    ")\n",
    "stations = stations.set_index('id')\n",
    "\n",
    "# the dutch stations in the PSMSL database, make a copy\n",
    "# or use stations.coastline_code == 150 for all dutch stations\n",
    "selected_stations = stations.ix[main_stations_idx].copy()\n",
    "# set the main stations, this should be a list of 6 stations\n",
    "# selected_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined which tide gauges we are monitoring we can start downloading the relevant data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each station has a number of files that you can look at.\n",
    "# here we define a template for each filename\n",
    "\n",
    "# stations that we are using for our computation\n",
    "# define the name formats for the relevant files\n",
    "names = {\n",
    "    'datum': '{dataset}/RLR_info/{id}.txt',\n",
    "    'diagram': '{dataset}/RLR_info/{id}.png',\n",
    "    'url': 'http://www.psmsl.org/data/obtaining/rlr.diagrams/{id}.php',\n",
    "    'data': '{dataset}/data/{id}.rlrdata',\n",
    "    'doc': '{dataset}/docu/{id}.txt',\n",
    "    'contact': '{dataset}/docu/{id}_auth.txt'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define some helper functions to parse retrieved PSMSL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(station, dataset):\n",
    "    \"\"\"return the url of the station information (diagram and datum)\"\"\"\n",
    "    info = dict(\n",
    "        dataset=dataset,\n",
    "        id=station.name\n",
    "    )\n",
    "    url = names['url'].format(**info)\n",
    "    return url\n",
    "\n",
    "\n",
    "# fill in the dataset parameter using the global dataset_name\n",
    "f = functools.partial(get_url, dataset=dataset_name)\n",
    "# compute the url for each station\n",
    "selected_stations['url'] = selected_stations.apply(f, axis=1)\n",
    "# selected_stations\n",
    "\n",
    "\n",
    "def missing2nan(value, missing=-99999):\n",
    "    \"\"\"\n",
    "    convert the value to nan if the float of value equals the missing value\n",
    "    \"\"\"\n",
    "    value = float(value)\n",
    "    if value == missing:\n",
    "        return np.nan\n",
    "    return value\n",
    "\n",
    "\n",
    "def year2date(year_fraction, dtype):\n",
    "    startpoints = np.linspace(0, 1, num=12, endpoint=False)\n",
    "    remainder = np.mod(year_fraction, 1)\n",
    "    year = np.floor_divide(year_fraction, 1).astype('int')\n",
    "    month = np.searchsorted(startpoints, remainder)\n",
    "    dates = [\n",
    "        datetime.datetime(year_i, month_i, 1)\n",
    "        for year_i, month_i in zip(year, month)\n",
    "    ]\n",
    "    datetime64s = np.asarray(dates, dtype=dtype)\n",
    "    return datetime64s\n",
    "\n",
    "\n",
    "def get_data(station, dataset):\n",
    "    \"\"\"get data for the station (pandas record) from the dataset (url)\"\"\"\n",
    "    info = dict(\n",
    "        dataset=dataset,\n",
    "        id=station.name\n",
    "    )\n",
    "    bytes = zf.read(names['data'].format(**info))\n",
    "    df = pandas.read_csv(\n",
    "        io.BytesIO(bytes),\n",
    "        sep=';',\n",
    "        names=('year', 'height', 'interpolated', 'flags'),\n",
    "        converters={\n",
    "            \"height\": lambda x: main_stations[station.name]['rlr2nap'](missing2nan(x)),\n",
    "            \"interpolated\": str.strip,\n",
    "        }\n",
    "    )\n",
    "    df['station'] = station.name\n",
    "    df['t'] = year2date(df.year, dtype=wind_df.t.dtype)\n",
    "    # merge the wind and water levels\n",
    "    merged = pandas.merge(df, wind_df, how='left', on='t')\n",
    "    merged['u2'] = np.where(np.isnan(merged['u']),\n",
    "                            np.nanmean(merged['u']**2),\n",
    "                            merged['u']**2)\n",
    "    merged['v2'] = np.where(np.isnan(merged['v']),\n",
    "                            np.nanmean(merged['v']**2),\n",
    "                            merged['v']**2)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the data from the PSMSL website for the selected stations and store waterlevel in DataFrame column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for all stations\n",
    "f = functools.partial(get_data, dataset=dataset_name)\n",
    "# look up the data for each station\n",
    "selected_stations['data'] = [f(station) for _, station in\n",
    "                             selected_stations.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis we use the data from 1930 onwards. Furthermore the data is transformed such that DataFrame has a single datetime index and multiple columns with the waterlevel data for each selected station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "names = []\n",
    "for id, station in selected_stations.iterrows():\n",
    "    df = station['data'].ix[station['data'].year >= 1930]\n",
    "    dfs.append(df.set_index('year')['height'])\n",
    "    names.append(station['name'])\n",
    "merged = pandas.concat(dfs, axis=1)\n",
    "merged.columns = names\n",
    "diffs = merged.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged.copy()  # merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For explorative analysis purposes the linear_model function is used to determine the trend of the result of the PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the statistical model\n",
    "def linear_model(df, with_wind=True, with_season=True):\n",
    "    y = df['height']\n",
    "    X = np.c_[\n",
    "        df['year']-1970,\n",
    "        np.cos(2*np.pi*(df['year']-1970)/18.613),\n",
    "        np.sin(2*np.pi*(df['year']-1970)/18.613)\n",
    "    ]\n",
    "    month = np.mod(df['year'], 1) * 12.0\n",
    "    names = ['Constant', 'Trend', 'Nodal U', 'Nodal V']\n",
    "    if with_wind:\n",
    "        X = np.c_[\n",
    "            X,\n",
    "            df['u2'],\n",
    "            df['v2']\n",
    "        ]\n",
    "        names.extend(['Wind U^2', 'Wind V^2'])\n",
    "    if with_season:\n",
    "        for i in range(11):\n",
    "            X = np.c_[\n",
    "                X,\n",
    "                np.logical_and(month >= i, month < i+1)\n",
    "            ]\n",
    "            names.append('month_%s' % (i+1, ))\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X, missing='drop')\n",
    "    fit = model.fit()\n",
    "    return fit, names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
