{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling autocorrelation in the sea level\n",
    "\n",
    "The statistical tests and model diagnostics show autocorrelation in the sea levels. This notebook tests the AR(1) specification in the model with storm surge and implicitly adds the flexibility of one or more structural breaks in the trend parameter. \n",
    "\n",
    "Note that we need the package PyFlux as the AR(1) model with explanatory variables is in the class of ARIMAX models. The package StatsModels cannot deal with explanatory variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicolai.HKV\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nbformat\\current.py:19: UserWarning: nbformat.current is deprecated.\n",
      "\n",
      "- use nbformat for read/write/validate public API\n",
      "- use nbformat.vX directly to composing notebooks of a particular version\n",
      "\n",
      "  \"\"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"a22ada92-5fc8-4129-beb5-c229d89f0392\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"a22ada92-5fc8-4129-beb5-c229d89f0392\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"a22ada92-5fc8-4129-beb5-c229d89f0392\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'a22ada92-5fc8-4129-beb5-c229d89f0392' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.16.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"a22ada92-5fc8-4129-beb5-c229d89f0392\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"a22ada92-5fc8-4129-beb5-c229d89f0392\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"a22ada92-5fc8-4129-beb5-c229d89f0392\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'a22ada92-5fc8-4129-beb5-c229d89f0392' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.16.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"a22ada92-5fc8-4129-beb5-c229d89f0392\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard Python packages\n",
    "import io\n",
    "\n",
    "# Python packages that need to be installed using pip or anaconda:\n",
    "# For computations\n",
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh.palettes\n",
    "import bokeh.plotting\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "# Initialize modules for the jupyter notebook format\n",
    "from nbformat import current\n",
    "%matplotlib inline\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "from IPython.display import display\n",
    "import shutil\n",
    "\n",
    "import statsmodels.graphics.regressionplots as plots\n",
    "\n",
    "# Disable pandas warnings\n",
    "pandas.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_notebook(nbfile):\n",
    "    \"\"\"Function to run other notebook in this notebook\"\"\"\n",
    "    with io.open(nbfile,encoding=\"utf8\") as f:\n",
    "        nb = current.read(f, 'json')\n",
    "    \n",
    "    ip = get_ipython()\n",
    "    \n",
    "    for cell in nb.worksheets[0].cells:\n",
    "        if cell.cell_type != 'code':\n",
    "            continue\n",
    "        ip.run_cell(cell.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load notebook with basic io functionality (wind, PSMSL) and standard linear model\n",
    "execute_notebook('../satellite/get-data.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rlr_annual 20 rlr\n",
      "rlr_annual 22 rlr\n",
      "rlr_annual 23 rlr\n",
      "rlr_annual 24 rlr\n",
      "rlr_annual 25 rlr\n",
      "rlr_annual 32 rlr\n"
     ]
    }
   ],
   "source": [
    "# laad eerste het model met de summary\n",
    "station_names = [\n",
    "    'Vlissingen', \n",
    "    'Hoek van Holland', \n",
    "    'Den Helder', \n",
    "    'Delfzijl', \n",
    "    'Harlingen', \n",
    "    'IJmuiden'\n",
    "]\n",
    "\n",
    "# Locatie wind data is 50 km uit kust vanuit IJmuiden\n",
    "rlr_data = get_station_data(dataset_name='rlr_annual', coastline_code=150, names=station_names, include_wind=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sea water level data of the following stations are analyzed: Vlissingen, Hoek van holland, Den helder, Delfzijl, Harlingen, Ijmuiden\n"
     ]
    }
   ],
   "source": [
    "stations = [20, 22, 23, 24, 25, 32]\n",
    "\n",
    "grouped = pandas.concat(rlr_data.loc[stations, 'data'].tolist())[['year', 'height']].groupby(['year'])\n",
    "mean_df = grouped.mean().reset_index()\n",
    "# filter out non-trusted part (before NAP)\n",
    "mean_df = mean_df[mean_df['year'] >= 1890]\n",
    "\n",
    "station_names = [st.capitalize() for st in rlr_data.loc[stations, 'name'].tolist()]\n",
    "\n",
    "print(f'The sea water level data of the following stations are analyzed: {\", \".join(station_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>1890</td>\n",
       "      <td>-194.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>1891</td>\n",
       "      <td>-179.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>1892</td>\n",
       "      <td>-166.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>1893</td>\n",
       "      <td>-142.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>1894</td>\n",
       "      <td>-141.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year      height\n",
       "year                  \n",
       "1890  1890 -194.666667\n",
       "1891  1891 -179.000000\n",
       "1892  1892 -166.500000\n",
       "1893  1893 -142.166667\n",
       "1894  1894 -141.833333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>2013</td>\n",
       "      <td>25.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>2014</td>\n",
       "      <td>66.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>2015</td>\n",
       "      <td>92.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>2016</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>2017</td>\n",
       "      <td>112.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year      height\n",
       "year                  \n",
       "2013  2013   25.666667\n",
       "2014  2014   66.666667\n",
       "2015  2015   92.500000\n",
       "2016  2016   77.000000\n",
       "2017  2017  112.166667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_df.index = mean_df['year']\n",
    "display(mean_df.head(), mean_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sea level data for the period\n",
      "1890 2017\n"
     ]
    }
   ],
   "source": [
    "# Sea level data range\n",
    "print(f'Sea level data for the period')\n",
    "print(min(mean_df.year.values), max(mean_df.year.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nicolai\n",
      "D:/Users/nicolai/Documents/2695.50 Zeespiegelstijging 2018/Data/\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()\n",
    "print(username)\n",
    "\n",
    "if username == 'rongen': \n",
    "    surgepath = 'D:/Documents/2695.50 Zeespiegelstijging 2018/Data/'\n",
    "elif username == 'nicolai':\n",
    "    surgepath = 'D:/Users/' + username + '/Documents/2695.50 Zeespiegelstijging 2018/Data/'\n",
    "    print(surgepath)\n",
    "else:\n",
    "#    print('enter path where surge is stored using forward slashes')\n",
    "#    surgepath = input()\n",
    "#    print(surgepath)\n",
    "    print('path not valid')\n",
    "\n",
    "#print(surgepath + 'surge.pkl')   \n",
    "\n",
    "#'D:\\Users\\Nicolai\\Documents\\2695.50 Zeespiegelstijging 2018\\Data'\n",
    "\n",
    "# Add surge\n",
    "# Load surge and convert from meters to mm\n",
    "#surge = pandas.read_pickle(surgepath + 'surge.pkl') * 1000\n",
    "#print(surge.head(5), surge.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Vlissingen  Hoek van holland  Den helder   Delfzijl  \\\n",
      "1979-01-01 00:00:00    0.000000          0.000000    0.000000   0.000000   \n",
      "1979-01-01 00:10:00   -0.249468          0.590325    0.806396  -3.677294   \n",
      "1979-01-01 00:20:00   -0.890010          1.760389    3.596331 -11.441492   \n",
      "1979-01-01 00:30:00   -1.303061          2.307753    7.541155 -16.544901   \n",
      "1979-01-01 00:40:00   -1.140522          1.127529   10.611323 -14.176514   \n",
      "\n",
      "                     Harlingen  Ijmuiden  \n",
      "1979-01-01 00:00:00   0.000000  0.000000  \n",
      "1979-01-01 00:10:00  -2.944402 -0.753033  \n",
      "1979-01-01 00:20:00  -5.254048 -2.288197  \n",
      "1979-01-01 00:30:00  -4.505036 -3.556560  \n",
      "1979-01-01 00:40:00  -5.693127 -4.844942                        Vlissingen  Hoek van holland  Den helder   Delfzijl  \\\n",
      "2014-12-31 23:10:00 -177.668601       -153.670296  -95.220782  15.352673   \n",
      "2014-12-31 23:20:00 -181.090802       -151.775345  -74.545339  10.494649   \n",
      "2014-12-31 23:30:00 -181.896344       -149.892956  -74.131303   6.929162   \n",
      "2014-12-31 23:40:00 -184.564605       -149.479315  -80.773443   2.179859   \n",
      "2014-12-31 23:50:00 -190.054506       -150.883034  -74.545391  -7.403774   \n",
      "\n",
      "                     Harlingen    Ijmuiden  \n",
      "2014-12-31 23:10:00 -68.900131 -110.704631  \n",
      "2014-12-31 23:20:00 -63.965112 -108.435169  \n",
      "2014-12-31 23:30:00 -56.788363 -105.522290  \n",
      "2014-12-31 23:40:00 -46.634056 -100.702301  \n",
      "2014-12-31 23:50:00 -41.788533  -96.721426  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicolai.HKV\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: 'year' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n"
     ]
    }
   ],
   "source": [
    "# Add surge\n",
    "# Load surge and convert from meters to mm\n",
    "\n",
    "# Guus\n",
    "# surge = pandas.read_pickle('D:/Documents/2695.50 Zeespiegelstijging 2018/Data/surge.pkl') * 1000 \n",
    "\n",
    "# Robin\n",
    "surgepath = 'D:/Users/Nicolai/Documents/2695.50 Zeespiegelstijging 2018/Data/'\n",
    "surge = pandas.read_pickle(surgepath + 'surge.pkl') * 1000\n",
    "\n",
    "print(surge.head(5), surge.tail(5))\n",
    "\n",
    "# Calculate average over given stations, per year\n",
    "# if monthly averages are needed, groupby \"surge.index.month\"\n",
    "average = surge[station_names].groupby(surge.index.year).mean().mean(axis=1)\n",
    "# Construct dataframe and add to mean_df\n",
    "surge_per_year = pandas.DataFrame(data=[average.mean()] * len(mean_df), index=mean_df['year'], columns=['surge'])\n",
    "surge_per_year.loc[average.index, 'surge'] = average.values\n",
    "surge_per_year.index.name = 'year'\n",
    "if 'surge' not in mean_df.columns:\n",
    "    mean_df = mean_df.merge(surge_per_year.reset_index(), on='year')\n",
    "\n",
    "# Create a corrected dataframe by subtracting the surge\n",
    "mean_df_corrected = mean_df.copy()\n",
    "mean_df_corrected['height'] -= mean_df_corrected['surge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storm surge data for the period\n",
      "1979 - 2014\n"
     ]
    }
   ],
   "source": [
    "# storm surge data range\n",
    "\n",
    "print(f'Storm surge data for the period')\n",
    "print(min(surge.index.year), '-', max(surge.index.year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the linear regression model with storm surge (no robust standard errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_with_surge(df):\n",
    "    \"\"\"\n",
    "    Return the fit from the linear model on the given dataset df.\n",
    "    Wind and season can be enabled and disabled\n",
    "    \"\"\"\n",
    "    y = df['height']\n",
    "    X = np.c_[\n",
    "        df['year']-1970, \n",
    "        np.cos(2*np.pi*(df['year']-1970)/18.613),\n",
    "        np.sin(2*np.pi*(df['year']-1970)/18.613),\n",
    "        df['surge']# * (df['year'] >= 1979)    \n",
    "    ]\n",
    "    month = np.mod(df['year'], 1) * 12.0\n",
    "    names = ['Constant', 'Trend', 'Nodal U', 'Nodal V', 'Surge']\n",
    "        \n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X, missing='drop')\n",
    "    fit = model.fit()\n",
    "    #fit = model.fit(cov_type='HC0')\n",
    "    #fit = model.fit(cov_type='HAC')\n",
    "    return fit, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Linear model (1890 - current), with surge (1979-current)</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>Sea-surface height</td> <th>  R-squared:         </th> <td>   0.892</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                    <td>OLS</td>        <th>  Adj. R-squared:    </th> <td>   0.889</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>              <td>Least Squares</td>   <th>  F-statistic:       </th> <td>   255.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>              <td>Fri, 14 Sep 2018</td>  <th>  Prob (F-statistic):</th> <td>1.54e-58</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                  <td>19:16:39</td>      <th>  Log-Likelihood:    </th> <td> -593.96</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>       <td>   128</td>       <th>  AIC:               </th> <td>   1198.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>           <td>   123</td>       <th>  BIC:               </th> <td>   1212.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>               <td>     4</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>       <td>nonrobust</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Constant</th> <td>  -23.1142</td> <td>    2.482</td> <td>   -9.313</td> <td> 0.000</td> <td>  -28.027</td> <td>  -18.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Trend</th>    <td>    1.9267</td> <td>    0.062</td> <td>   31.324</td> <td> 0.000</td> <td>    1.805</td> <td>    2.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Nodal U</th>  <td>    4.5272</td> <td>    3.210</td> <td>    1.410</td> <td> 0.161</td> <td>   -1.827</td> <td>   10.881</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Nodal V</th>  <td>  -10.8684</td> <td>    3.212</td> <td>   -3.384</td> <td> 0.001</td> <td>  -17.226</td> <td>   -4.511</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Surge</th>    <td>    0.9018</td> <td>    0.144</td> <td>    6.259</td> <td> 0.000</td> <td>    0.617</td> <td>    1.187</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 3.042</td> <th>  Durbin-Watson:     </th> <td>   1.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.219</td> <th>  Jarque-Bera (JB):  </th> <td>   2.517</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.258</td> <th>  Prob(JB):          </th> <td>   0.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.454</td> <th>  Cond. No.          </th> <td>    57.8</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "           Linear model (1890 - current), with surge (1979-current)           \n",
       "==============================================================================\n",
       "Dep. Variable:     Sea-surface height   R-squared:                       0.892\n",
       "Model:                            OLS   Adj. R-squared:                  0.889\n",
       "Method:                 Least Squares   F-statistic:                     255.2\n",
       "Date:                Fri, 14 Sep 2018   Prob (F-statistic):           1.54e-58\n",
       "Time:                        19:16:39   Log-Likelihood:                -593.96\n",
       "No. Observations:                 128   AIC:                             1198.\n",
       "Df Residuals:                     123   BIC:                             1212.\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Constant     -23.1142      2.482     -9.313      0.000     -28.027     -18.201\n",
       "Trend          1.9267      0.062     31.324      0.000       1.805       2.048\n",
       "Nodal U        4.5272      3.210      1.410      0.161      -1.827      10.881\n",
       "Nodal V      -10.8684      3.212     -3.384      0.001     -17.226      -4.511\n",
       "Surge          0.9018      0.144      6.259      0.000       0.617       1.187\n",
       "==============================================================================\n",
       "Omnibus:                        3.042   Durbin-Watson:                   1.505\n",
       "Prob(Omnibus):                  0.219   Jarque-Bera (JB):                2.517\n",
       "Skew:                          -0.258   Prob(JB):                        0.284\n",
       "Kurtosis:                       3.454   Cond. No.                         57.8\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "linear_fit_with_surge, names = linear_model_with_surge(mean_df)\n",
    "table = linear_fit_with_surge.summary(\n",
    "    yname='Sea-surface height', \n",
    "    xname=names, \n",
    "    title='Linear model (1890 - current), with surge (1979-current)'\n",
    ")\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Durbin-Watson test, Breusch-Godfrey test, and (partial) autocorrelation function show that this model exhibits autocorrelation with lag 1 [see notebook Statistical Tests]. Other literature on sea level regression finds this behaviour as well. Furthermore, the residuals are heteroskedastic, i.e. they depend on the value of the explanatory variables. One way to deal with both issues is estimating robust standard errors. The HAC estimator takes into account both heteroskedasticity and autocorrelation [see notebook Statistical Tests]. Below we shall introduce an AR(1) model to capture the autocorrelation in the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const       -135.640573\n",
      "trend          1.467085\n",
      "L1.height      0.230480\n",
      "dtype: float64\n",
      "\n",
      "confidence interval\n",
      "[[-1.67319629e+02 -1.03961518e+02]\n",
      " [ 1.12058877e+00  1.81358144e+00]\n",
      " [ 6.07140011e-02  4.00245960e-01]]\n",
      "\n",
      "standard errors\n",
      "[16.16308042  0.1767871   0.08661689]\n",
      "\n",
      "t-values\n",
      "const       -8.392000\n",
      "trend        8.298598\n",
      "L1.height    2.660913\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-7.932728955793214,\n",
       " 1.0222059792980013e-10,\n",
       " 1,\n",
       " 126,\n",
       " {'1%': -4.032477692253856,\n",
       "  '5%': -3.4459262881778225,\n",
       "  '10%': -3.1478290513383484},\n",
       " 1216.7811025960364)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import statsmodels.tsa.ar_model as ar_model\n",
    "import statsmodels.tsa.stattools as ar_tools\n",
    "\n",
    "#mean_df.head()\n",
    "ar = ar_model.AR(mean_df['height'])\n",
    "#print(ar.information(model_ar.params))\n",
    "\n",
    "model_ar = ar.fit(method='mle', maxlag=1, trend='ct')\n",
    "print(model_ar.params)\n",
    "\n",
    "ar_results = ar_model.ARResults(ar, model_ar.params)\n",
    "\n",
    "print(\"\")\n",
    "print(\"confidence interval\")\n",
    "print(ar_results.conf_int())\n",
    "\n",
    "print(\"\")\n",
    "print(\"standard errors\")\n",
    "print(ar_results.bse)\n",
    "\n",
    "print(\"\")\n",
    "print(\"t-values\")\n",
    "print(ar_results.tvalues)\n",
    "      \n",
    "ar_tools.adfuller(mean_df['height'], maxlag = 1, regression='ct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AR(1) model with constant and trend (without minus 1970) leads to a smaller trend. \n",
    "\n",
    "The AR(1) parameter does not seem to be equal to 1. The null hypothesis of the Augmented Dickey-Fuller test, there is a unit root, is rejected. \n",
    "\n",
    "Now we will add the other variables to the equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyflux as pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_AR_model_with_surge(df):\n",
    "    \"\"\"\n",
    "    Return the fit from the (linear) ARX(1) model on the given dataset df.\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    df2['height'] = df2['height']\n",
    "    df2['nodalcos'] = np.cos(2*np.pi*(df2['year']-1970)/18.613)\n",
    "    df2['nodalsin'] = np.sin(2*np.pi*(df2['year']-1970)/18.613)\n",
    "    df2['trend'] = (df2['year']-1970)\n",
    "    df2['surge'] = df2['surge']\n",
    "    \n",
    "    print(df2.head())\n",
    "    \n",
    "    names = ['AR(1)', 'Constant', 'Trend', 'Nodal U', 'Nodal V', 'Surge']   \n",
    "    \n",
    "    model = pf.ARIMAX(data=df2, formula='height~1+trend+nodalcos+nodalsin+surge',\n",
    "                  ar=1, ma=0, integ=0,family=pf.Normal())\n",
    "  \n",
    "    fit = model.fit(\"MLE\",cov_type='HC0')\n",
    "    #fit = model.fit(\"MLE\")\n",
    "    return model, fit, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year      height     surge\n",
      "0  1890 -194.666667 -0.960549\n",
      "1  1891 -179.000000 -0.960549\n",
      "2  1892 -166.500000 -0.960549\n",
      "3  1893 -142.166667 -0.960549\n",
      "4  1894 -141.833333 -0.960549\n",
      "   year      height     surge  nodalcos  nodalsin  trend\n",
      "0  1890 -194.666667 -0.960549 -0.297469 -0.954731    -80\n",
      "1  1891 -179.000000 -0.960549  0.035522 -0.999369    -79\n",
      "2  1892 -166.500000 -0.960549  0.364503 -0.931202    -78\n",
      "3  1893 -142.166667 -0.960549  0.652341 -0.757926    -77\n",
      "4  1894 -141.833333 -0.960549  0.866545 -0.499098    -76\n",
      "Normal ARIMAX(1,0,0)                                                                                      \n",
      "======================================================= ==================================================\n",
      "Dependent Variable: height                              Method: MLE                                       \n",
      "Start Date: 1                                           Log Likelihood: -587.3706                         \n",
      "End Date: 127                                           AIC: 1188.7413                                    \n",
      "Number of observations: 127                             BIC: 1208.6506                                    \n",
      "==========================================================================================================\n",
      "Latent Variable                          Estimate   Std Error  z        P>|z|    95% C.I.                 \n",
      "======================================== ========== ========== ======== ======== =========================\n",
      "AR(1)                                    0.1508     0.0767     1.9651   0.0494   (0.0004 | 0.3012)        \n",
      "Beta 1                                   -19.0425   3.1482     -6.0487  0.0      (-25.213 | -12.8721)     \n",
      "Beta trend                               1.6298     0.1582     10.3034  0.0      (1.3197 | 1.9398)        \n",
      "Beta nodalcos                            2.9095     3.1891     0.9123   0.3616   (-3.3413 | 9.1602)       \n",
      "Beta nodalsin                            -9.8069    3.2034     -3.0614  0.0022   (-16.0856 | -3.5282)     \n",
      "Beta surge                               0.899      0.1391     6.4624   0.0      (0.6263 | 1.1717)        \n",
      "Normal Scale                             24.6844                                                          \n",
      "==========================================================================================================\n",
      "   year      height     surge\n",
      "0  1890 -194.666667 -0.960549\n",
      "1  1891 -179.000000 -0.960549\n",
      "2  1892 -166.500000 -0.960549\n",
      "3  1893 -142.166667 -0.960549\n",
      "4  1894 -141.833333 -0.960549\n"
     ]
    }
   ],
   "source": [
    "print(mean_df.head())\n",
    "ar_model, ar_fit_with_surge, names = linear_AR_model_with_surge(mean_df)\n",
    "ar_fit_with_surge.summary()\n",
    "\n",
    "#table = ar_fit_with_surge.summary(\n",
    "#    yname='Sea-surface height', \n",
    "#    xname=names, \n",
    "#    title='Linear AR(1) model (1890 - current), with surge (1979-current)'\n",
    "#)\n",
    "#display(table)\n",
    "print(mean_df.head()) # Why does mean_df change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit = model.fit('MLE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit = model.fit('MLE',covtype='HC0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ARX(1) model fits the model quite well. The trend parameter is smaller than it was in the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_model.plot_fit(figsize=(15,10))\n",
    "ar_model.plot_predict(h=10, oos_data=mean_df.iloc[-12:], past_values=100, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model diagnostics : to do for the ARX(1) model\n",
    "## Normality\n",
    "Let us first check if the (fitted) residuals are normally distributed. We do several visual checks and consider statistical tests.\n",
    "\n",
    "### Statistical test\n",
    "The Jarque-Bera test statistic is 2.517 with p-value 0.284. Hence, the null hypothesis of normality cannot be rejected at a significance level of 5%. [To do: including numbers from table in text.]\n",
    "\n",
    "### Q-Q-plot of the residuals\n",
    "Next we create the quantile-quantile plot of the residuals. That is, we plot the theoretical quantiles (normal distribution) against the data quantiles (ranks).\n",
    "\n",
    "The first plot relates to the residuals of the full model, the second plot to the residuals from 1979 from the full model and the third plot relates to the residuals of the model fitted to the data from 1979 onwards.\n",
    "\n",
    "The Q-Q-plots do not show large irregularities in the residuals. Higher quantiles (0.7-0.9) are somewhat overrepresented and the quantiles 0.3-0.5 are somewhat underrepresented (first plot, full model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(14, 4), ncols=3)\n",
    "from scipy.stats import norm\n",
    "\n",
    "for startyear, ax, fit, title in zip(\n",
    "    [1890, 1979, 1979],\n",
    "    axs,\n",
    "    [linear_fit_with_surge, linear_fit_with_surge, linear_fit_with_surge_1979],\n",
    "    ['All residuals from fit 1890-2016', 'Residuals 1979-2016 from fit 1890-2016', 'Residuals from fit 1979-2016']\n",
    "):\n",
    "    \n",
    "    years = fit.model.exog[:, 1] + 1970\n",
    "    index = (years >= startyear)\n",
    "#     print(index)\n",
    "\n",
    "    \n",
    "    ax.set_aspect(1.0)\n",
    "\n",
    "    # Calculate the exceedance probabilities given a normal distribution\n",
    "    residuals = fit.resid.values[index]\n",
    "    cprob = norm.cdf(\n",
    "        sorted(residuals),\n",
    "        loc=residuals.mean(),\n",
    "        scale=residuals.std()\n",
    "    )\n",
    "    # Calculate the data quantiles: rank / (n + 1)\n",
    "#     print(fit.resid.rank().values)\n",
    "#     print(np.argsort(residuals))\n",
    "    ranks = (np.arange(len(residuals)) + 1) / (len(residuals) + 1)\n",
    "\n",
    "    # Scatter\n",
    "    ax.plot(cprob, ranks, marker='.', ls='')\n",
    "    # Plot the 1-1 line\n",
    "    ax.plot([0, 1], [0, 1], ls='-', color='0.5')\n",
    "\n",
    "    # Layout\n",
    "    ax.grid()\n",
    "    ax.set_ylabel('Normal data quantiles')\n",
    "    ax.set_xlabel('Normal theoretical quantiles')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    ax.set_title(title, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "\n",
    "norm_resid = (linear_fit_with_surge.resid - linear_fit_with_surge.resid.mean()) / linear_fit_with_surge.resid.std() \n",
    "\n",
    "n, bins, patches = plt.hist(norm_resid, density = True)\n",
    "plt.ylabel('Density')\n",
    "plt.xlabel('Normalized residuals');\n",
    "\n",
    "# best fit of data\n",
    "(mu, sigma) = norm.fit(norm_resid)\n",
    "\n",
    "# the histogram of the data\n",
    "#n, bins, patches = plt.hist(datos, 60, normed=1, facecolor='green', alpha=0.75)\n",
    "\n",
    "# add a 'best fit' line\n",
    "y = mlab.normpdf(bins, mu, sigma)\n",
    "l = plt.plot(bins, y, 'r--', linewidth=2)\n",
    "\n",
    "plt.title(r'$\\mathrm{Histogram\\ and\\ density\\ fit\\ of\\ normalized\\ residuals:}\\ \\mu=%.3f,\\ \\sigma=%.3f$' %(mu, sigma))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the normalized residuals of the full model is close to a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "Outliers can have an impact on the parameter estimates and the fitted residuals. Below we first plot the fitted sea levels including confidence and prediction intervals. Next we plot the residuals against time and against leverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code om betrouwbaarheidsintervallen en predictie-intervallen te maken\n",
    "\n",
    "# Confidence intervals paramemeters using t-distribution\n",
    "# print(linear_fit_with_surge.conf_int(alpha=0.05, cols=None)) \n",
    "## linear_fit_with_surge.conf_int_el # using empirical likelihood\n",
    "\n",
    "from statsmodels.stats.outliers_influence import summary_table, OLSInfluence\n",
    "lfws = linear_fit_with_surge\n",
    "st, data, ss2 = summary_table(lfws, alpha=0.05)\n",
    "#print(lfws.model.nobs)\n",
    "#print(lfws.model.exog_names)\n",
    "\n",
    "fittedvalues = data[:,2]\n",
    "predict_mean_se  = data[:,3]\n",
    "predict_mean_ci_low, predict_mean_ci_upp = data[:,4:6].T # confidence interval\n",
    "predict_ci_low, predict_ci_upp = data[:,6:8].T # prediction interval\n",
    "\n",
    "# check we got the right things\n",
    "#print(fittedvalues)\n",
    "#print(np.max(np.abs(lfws.fittedvalues - fittedvalues)))\n",
    "#print(np.max(np.abs(iv_l - predict_ci_low))) #iv_l and iv_u are lower and upper values prediction interval from another method\n",
    "#print(np.max(np.abs(iv_u - predict_ci_upp))) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model with storm surge and add confidence and prediction intervals\n",
    "# grafieken van de predictie-intervallen van het regressiemodel en de waarnemingen.\n",
    "for startyear in [1890, 1979]:\n",
    "\n",
    "    fig = bokeh.plotting.figure(x_range=(startyear, 2020), plot_width=900, plot_height=400)\n",
    "\n",
    "    fig.circle(\n",
    "        mean_df.year,\n",
    "        mean_df.height,\n",
    "        line_width=1,\n",
    "        legend='Annual sea level (data)',\n",
    "        color='black',\n",
    "        alpha=0.5\n",
    "    )\n",
    "\n",
    "    fig.line(\n",
    "        linear_fit_with_surge.model.exog[:, 1] + 1970, \n",
    "        linear_fit_with_surge.predict(), \n",
    "        line_width=3, \n",
    "        legend='Current sea level (linear model with surge)', \n",
    "        color='black',\n",
    "        alpha=0.5\n",
    "    )\n",
    "    \n",
    "    for values, label, color, lw in zip([predict_ci_low, predict_ci_upp, predict_mean_ci_low, predict_mean_ci_upp],\n",
    "                             ['95% prediction interval', '', '95% confidence interval', ''],\n",
    "                             ['blue', 'blue', 'red', 'red'],\n",
    "                             [2, 2, 1, 1]):\n",
    "        fig.line(\n",
    "            linear_fit_with_surge.model.exog[:, 1] + 1970, \n",
    "            values, \n",
    "            line_width=lw, \n",
    "            legend=label, \n",
    "            color=color,\n",
    "            alpha=0.5\n",
    "        )\n",
    "\n",
    "    fig.legend.location = \"bottom_right\"\n",
    "    fig.yaxis.axis_label = 'water level [mm] above NAP'\n",
    "    fig.xaxis.axis_label = 'year'\n",
    "    #fig.legend.click_policy = \"hide\"\n",
    "\n",
    "    bokeh.io.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a few observations are outside the prediction intervals. It is remarkable to see so many observations in the confidence interval. The sea level in year 1996 is comparatively low. The fit is good though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Controle op uitbijters (outlier detection): standaard residuenplot \n",
    "\n",
    "# residual plot\n",
    "fig = bokeh.plotting.figure(x_range=(1890, 2020), plot_width=900, plot_height=400)\n",
    "fig.circle(\n",
    "    mean_df.year,\n",
    "    linear_fit_with_surge.resid,\n",
    "    line_width=1,\n",
    "    legend='Residuals',\n",
    "    color='black',\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "fig.legend.location = \"top_left\"\n",
    "fig.yaxis.axis_label = 'water level [mm] above NAP'\n",
    "fig.xaxis.axis_label = 'year'\n",
    "#fig.legend.click_policy = \"hide\"\n",
    "\n",
    "bokeh.io.show(fig)\n",
    "\n",
    "print('The graph shows a sort of cyclical pattern in the residuals implying autocorrelation.')\n",
    "\n",
    "print('From 2005 onwards the residuals increase. The residuals between 2014-2017 are relatively large, but in the early 1960s the residuals have been large as well.') \n",
    "\n",
    "print('The lack of storm surge data from 2015 onwards is likely to contribute to the high values in 2015-2017.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# influence = linear_fit_with_surge_1979.get_influence()\n",
    "# frame = influence.summary_frame()\n",
    "# display(frame.sort_values(by='hat_diag', ascending=False).head())\n",
    "\n",
    "# print(linear_fit_with_surge.model.exog[106, 1] + 1970)\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# ax.scatter(frame['hat_diag'], frame['standard_resid'], s=20, alpha=0.5)\n",
    "# ax.set_ylabel('Standardized residuals')\n",
    "# ax.set_xlabel('Leverage');\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(14, 8), ncols=2)\n",
    "plots.plot_leverage_resid2(linear_fit_with_surge, ax=axs[0]);\n",
    "plots.plot_leverage_resid2(linear_fit_with_surge_1979, ax=axs[1]);\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_ylim(0.0, 0.35)\n",
    "    ax.set_xlim(0.0, 10.0)\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(14, 8), ncols=2)\n",
    "plots.influence_plot(linear_fit_with_surge, ax=axs[0]);\n",
    "plots.influence_plot(linear_fit_with_surge_1979, ax=axs[1]);\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_ylim(-2.5, 2.5)\n",
    "    ax.set_xlim(0.0, 0.35)\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plots show that the year 1996 has the highest leverage. The influence is relatively high, but some other years with the same leverage have the same influence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial correlation: Breusch-Godfrey test\n",
    "The null hypothesis of the Breusch-Godfrey test is no serial correlation.\n",
    "\n",
    "#### References\n",
    "Breusch, T. S. (1978). \"Testing for Autocorrelation in Dynamic Linear Models\". Australian Economic Papers. 17: 334–355. doi:10.1111/j.1467-8454.1978.tb00635.x.\n",
    "\n",
    "Godfrey, L. G. (1978). \"Testing Against General Autoregressive and Moving Average Error Models when the Regressors Include Lagged Dependent Variables\". Econometrica. 46: 1293–1301. JSTOR 1913829."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels\n",
    "\n",
    "fig = bokeh.plotting.figure(x_range=(1890, 2020), y_range=(-80, 80), plot_width=900, plot_height=400)\n",
    "\n",
    "resid_df = pd.DataFrame(\n",
    "    data=linear_fit_with_surge.resid.values,\n",
    "    index=mean_df_corrected.year,\n",
    "    columns=['residuals']\n",
    ")\n",
    "resid_df['5 year rolling average'] = resid_df.rolling(window=5, center=True).mean()\n",
    "\n",
    "fig.line(resid_df.index.values, resid_df['residuals'].values,\n",
    "         line_width=2, color='blue', alpha=0.5, legend='Residuals')\n",
    "fig.line(resid_df.index.values, resid_df['5 year rolling average'].values,\n",
    "         line_width=2, color='red', alpha=0.5, legend='Moving average (5 years)')\n",
    "\n",
    "\n",
    "fig.legend.location = \"bottom_right\"\n",
    "fig.yaxis.axis_label = 'water level [mm] above NAP'\n",
    "fig.xaxis.axis_label = 'year'\n",
    "\n",
    "bokeh.io.show(fig)\n",
    "\n",
    "# statsmodels.stats.diagnostic.acorr_breusch_godfrey?\n",
    "bg = statsmodels.stats.diagnostic.acorr_breusch_godfrey(linear_fit_with_surge)\n",
    "\n",
    "print(\"\"\"\n",
    "Lagrange multiplier test statistic: {:.3f}\n",
    "P-value for Lagrange multiplier test: {:.3f}\n",
    "Fstatistic for F test: {:.3f}\n",
    "P-value for F test: {:.3f}\n",
    "\"\"\".format(*bg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis \"No serial correlation\" is rejected at the 5% significance level. This is in line with the outcome of the Durbin-Watson test (on autocorrelation). This conclusion means that the covariance matrix and hence the standard errors might be too small.\n",
    "\n",
    "One way of dealing with autocorrelation is estimating the standard errors in a more robust way (White). \n",
    "\n",
    "Another way of dealing with autocorrelation is using an AR(p) model. Therefore, we shall first check the autocorrelation function to see what the lag is in the dependent variable.\n",
    "\n",
    "Remark. In case of autocorrelation and heteroscedasticity one can apply the HAC estimator to retrieve robust standard errors (statsmodels.stats.sandwich_covariance.cov_hac; Newey-West covariance matrix estimator). In the last section of this notebook we  test on homoscedasticity and implement robust standard errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorr(x):\n",
    "    result = np.correlate(x, x, mode='full')\n",
    "    return result[np.int(result.size/2):]\n",
    "\n",
    "plt.plot(autocorr(resid_df['residuals'].values))\n",
    "\n",
    "#from pandas import Series\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "# Remark: \n",
    "# ACF from residuals and mean sea level\n",
    "plot_acf(resid_df['residuals'].values, lags=10)\n",
    "pyplot.show()\n",
    "\n",
    "plot_acf(mean_df.height.values, lags=10)\n",
    "pyplot.show()\n",
    "\n",
    "# PACF from residuals and mean sea level\n",
    "plot_pacf(resid_df['residuals'].values, lags=10)\n",
    "pyplot.show()\n",
    "\n",
    "plot_pacf(mean_df.height.values, lags=10)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ACF and partial ACF of the residuals and the sea levels show autoregression with lag 1. This is in line with the outcomes of the statistical tests on autocorrelation and serial correlation. We could estimate an AR(1) model to deal with the autocorrelation. Neglecting autocorrelation could lead to incorrect standard errors. Another option is 'correcting' the standard errors for autocorrelation (and other types misspecification - heteroscedasticity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homoscedastic or heteroskedastic residuals: \n",
    "#### Breusch-Pagan test\n",
    "The null hypothesis of the Breusch-Pagan and White test is homoscedastic residuals. The White test is a special case of the B-P test.\n",
    "\n",
    "Breusch, T. S.; Pagan, A. R. (1979). \"A Simple Test for Heteroskedasticity and Random Coefficient Variation\". Econometrica. 47 (5): 1287–1294. doi:10.2307/1911963. JSTOR 1911963. MR 0545960.\n",
    "\n",
    "Cook, R. D.; Weisberg, S. (1983). \"Diagnostics for Heteroskedasticity in Regression\". Biometrika. 70 (1): 1–10. doi:10.1093/biomet/70.1.1.\n",
    "\n",
    "White, H. (1980). \"A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity\". Econometrica. 48 (4): 817–838. JSTOR 1912934. MR 0575027.\n",
    "\n",
    "First we plot the residuals against the main independent variables: nodal cycle and surge. Subsequently we perform the B-P test and the White test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nodal(C, D, year):\n",
    "    \"\"\"\n",
    "    C : float\n",
    "        Coefficient from the sin-component\n",
    "    D : float\n",
    "        Coefficient from the cos-component\n",
    "    \"\"\"\n",
    "    \n",
    "    # Recreate the nodal cycle from the sinus and cosinus components.\n",
    "    # It does not matter that we use the time-independent parameters.\n",
    "    phi = np.arctan(D / C)\n",
    "    A = (C**2 + D**2) ** 0.5\n",
    "    Tn = 18.613\n",
    "    \n",
    "    return A * np.sin((2 * np.pi * year) / Tn + phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3, nrows=2, figsize=(14, 10))\n",
    "\n",
    "for i, (yearmin, fit, title) in enumerate(zip(\n",
    "    [1890, 1979, 1979],\n",
    "    [linear_fit_with_surge, linear_fit_with_surge, linear_fit_with_surge_1979],\n",
    "    ['All residuals from fit 1890-2017', 'Residuals 1979-2017 from fit 1890-2017', 'All residuals from fit 1979-2017']\n",
    ")):\n",
    "\n",
    "    # Select years\n",
    "    indices = mean_df['year'] >= yearmin\n",
    "\n",
    "    # Get surge and residuals for fit and years\n",
    "    surge = mean_df.loc[indices, 'surge'].values\n",
    "    residuals = fit.resid.tail(len(surge)).values\n",
    "    \n",
    "    # Plot surge\n",
    "    ax = axs[0, i]\n",
    "    ax.scatter(surge, residuals, s=10)\n",
    "    ax.set_xlabel('Surge [mm]')\n",
    "    ax.set_xlim(-75, 75)\n",
    "    ax.set_aspect(1.0)\n",
    "    \n",
    "    # Format\n",
    "    for ax in axs[:, i]:\n",
    "        ax.grid()\n",
    "        ax.set_ylabel('Residuen [mm]')\n",
    "        ax.set_ylim(-75, 75)    \n",
    "    \n",
    "    # Nodal\n",
    "    C, D = linear_fit_with_surge.params.x2, linear_fit_with_surge.params.x3\n",
    "    nodal_pred = predict_nodal(C, D, mean_df.loc[indices, 'year'])\n",
    "    ax = axs[1, i]\n",
    "    ax.scatter(nodal_pred, residuals, s=10)\n",
    "    ax.set_xlabel('Nodal cyclus [mm]')\n",
    "    ax.set_xlim(-15, 15)\n",
    "    ax.set_aspect(0.2)\n",
    "\n",
    "    # Breusch-Pagan test\n",
    "    bp = statsmodels.stats.diagnostic.het_breuschpagan(residuals, mean_df.loc[indices].values)\n",
    "\n",
    "    print(\"\"\"\n",
    "    {}:\n",
    "    ---------------------------------------\n",
    "    Lagrange multiplier test statistic: {:.3f}\n",
    "    P-value for Lagrange multiplier test: {:.3f}\n",
    "    Fstatistic for F test: {:.3f}\n",
    "    P-value for F test: {:.3f}\n",
    "    \"\"\".format(title, *bp))\n",
    "    \n",
    "    # https://www.statsmodels.org/dev/stats.html   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots show that the spread in the residuals somewhat depends on the value of the nodal cycle and the surge. Therefore it is not suprising that the null hypothesis of the Breusch-Pagan test is rejected at the 5% significance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remark. The White test and Goldfeld-Quandt test are not yet applicable in Python's statsmodels module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # White test\n",
    "    wt = statsmodels.stats.diagnostic.het_white(residuals, mean_df.loc[indices].values)\n",
    "\n",
    "    print(\"\"\"\n",
    "    {}:\n",
    "    ---------------------------------------\n",
    "    Lagrange multiplier test statistic: {:.3f}\n",
    "    P-value for Lagrange multiplier test: {:.3f}\n",
    "    Fstatistic for F test: {:.3f}\n",
    "    P-value for F test: {:.3f}\n",
    "    \"\"\".format(title, *wt))\n",
    "    \n",
    "    # Goldfeld-Quandt test\n",
    "    gq = statsmodels.stats.diagnostic.het_goldfeldquandt(residuals, mean_df.loc[indices].values)\n",
    "\n",
    "    print(\"\"\"\n",
    "    {}:\n",
    "    ---------------------------------------\n",
    "    Lagrange multiplier test statistic: {:.3f}\n",
    "    P-value for Lagrange multiplier test: {:.3f}\n",
    "    Fstatistic for F test: {:.3f}\n",
    "    P-value for F test: {:.3f}\n",
    "    \"\"\".format(title, *gq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residuals vs. fitted values\n",
    "The same conclusions are drawn after plotting the residuals against the fitted sea water levels. The spread is higher for the lower fitted sea levels. This is because the lower fitted values relate to the begin period and in that period (until 1979) the surge does not explain any variance. Adding the surge to the model from the year 1979 onwards yields lower variation in the residuals. Still, the residuals show more variation for the lower and higher fitted values.\n",
    "\n",
    "This leads to the final conclusion that the model describes the data well and that most model assumptions are reasonable. There is some evidence of serial corrrelation and heteroscedasticity in the residuals. Therefore the model requires robust estimation of standard errors. This can be easily incorporated with the option \"cov_type='HC0' or \"cov_type='hac'\" in the function \"model.fit()\". For example, \"fit = model.fit(cov_type='HC0')\". The two covariance estimators are applied below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3, figsize=(13, 6))\n",
    "\n",
    "for i, (yearmin, fit, title) in enumerate(zip(\n",
    "    [1890, 1979, 1979],\n",
    "    [linear_fit_with_surge, linear_fit_with_surge, linear_fit_with_surge_1979],\n",
    "    ['All residuals from fit 1890-2017', 'Residuals 1979-2017 from fit 1890-2017', 'All residuals from fit 1979-2017']\n",
    ")):\n",
    "\n",
    "    # Select years\n",
    "    indices = mean_df['year'] >= yearmin\n",
    "\n",
    "    # Get surge and residuals for fit and years\n",
    "    predicted = fit.fittedvalues[indices]\n",
    "    residuals = fit.resid.tail(len(predicted)).values\n",
    "    \n",
    "    # Plot surge\n",
    "    ax = axs.ravel()[i]\n",
    "    ax.scatter(predicted, residuals, s=10)\n",
    "    ax.set_xlabel('Fitted [mm + NAP]')\n",
    "    ax.set_xlim(-200, 200)\n",
    "    ax.set_aspect(1.0)\n",
    "    \n",
    "    ax.grid()\n",
    "    ax.set_ylabel('Residual [mm]')\n",
    "    ax.set_ylim(-100, 100)   \n",
    "    \n",
    "    ax.set_title(title)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_with_surge_robust_hc0(df):\n",
    "    \"\"\"\n",
    "    Return the fit from the linear model on the given dataset df.\n",
    "    Wind and season can be enabled and disabled\n",
    "    \"\"\"\n",
    "    y = df['height']\n",
    "    X = np.c_[\n",
    "        df['year']-1970, \n",
    "        np.cos(2*np.pi*(df['year']-1970)/18.613),\n",
    "        np.sin(2*np.pi*(df['year']-1970)/18.613),\n",
    "        df['surge']# * (df['year'] >= 1979)    \n",
    "    ]\n",
    "    month = np.mod(df['year'], 1) * 12.0\n",
    "    names = ['Constant', 'Trend', 'Nodal U', 'Nodal V', 'Surge']\n",
    "        \n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X, missing='drop')\n",
    "    #fit = model.fit()\n",
    "    fit = model.fit(cov_type='HC0')\n",
    "    return fit, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(table) # summary table regression results with 'uncorrected' standard errors\n",
    "\n",
    "linear_fit_with_surge, names = linear_model_with_surge_robust_hc0(mean_df)\n",
    "table_hc0 = linear_fit_with_surge.summary(\n",
    "    yname='Sea-surface height', \n",
    "    xname=names, \n",
    "    title='Linear model (1890 - current), with surge (1979-current)'\n",
    ")\n",
    "display(table_hc0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_with_surge_robust_hac(df):\n",
    "    \"\"\"\n",
    "    Return the fit from the linear model on the given dataset df.\n",
    "    Wind and season can be enabled and disabled\n",
    "    \"\"\"\n",
    "    y = df['height']\n",
    "    X = np.c_[\n",
    "        df['year']-1970, \n",
    "        np.cos(2*np.pi*(df['year']-1970)/18.613),\n",
    "        np.sin(2*np.pi*(df['year']-1970)/18.613),\n",
    "        df['surge']# * (df['year'] >= 1979)    \n",
    "    ]\n",
    "    month = np.mod(df['year'], 1) * 12.0\n",
    "    names = ['Constant', 'Trend', 'Nodal U', 'Nodal V', 'Surge']\n",
    "        \n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X, missing='drop')\n",
    "    fit = model.fit(cov_type='HAC', cov_kwds={'maxlags':1}) # 1 lag (see ACF)\n",
    "    return fit, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_fit_with_surge, names = linear_model_with_surge_robust_hac(mean_df)\n",
    "table_hac = linear_fit_with_surge.summary(\n",
    "    yname='Sea-surface height', \n",
    "    xname=names, \n",
    "    title='Linear model (1890 - current), with surge (1979-current)'\n",
    ")\n",
    "display(table_hac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
