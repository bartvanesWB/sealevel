{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common functions and models\n",
    "\n",
    "This notebook contains functions and models that are used in multiple other sea level rise notebooks. To avoid repeating these functions in all notebooks, they are defined here.\n",
    "\n",
    "For an example of how to run this notebook, see: extended-data-sources.ipynb.\n",
    "\n",
    "Currently the functions present in this notebook are:\n",
    "1. A set of functions that together retrieve tide gauge records of the sea level.\n",
    "2. The linear statistical model to fit through a measured sea level series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is a list of packages that are used in this notebook\n",
    "# these come with python\n",
    "import io\n",
    "import zipfile\n",
    "import functools\n",
    "import bisect\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "# you can install these packages using pip or anaconda\n",
    "# (requests numpy pandas bokeh pyproj statsmodels)\n",
    "\n",
    "# for downloading\n",
    "import requests\n",
    "import netCDF4\n",
    "\n",
    "# computation libraries\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "# statistics\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a number of variables (global) with the location of content to download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the urls for the three PSMSL datasets\n",
    "urls = {\n",
    "    'met_monthly': 'http://www.psmsl.org/data/obtaining/met.monthly.data/met_monthly.zip',\n",
    "    'rlr_monthly': 'http://www.psmsl.org/data/obtaining/rlr.monthly.data/rlr_monthly.zip',\n",
    "    'rlr_annual': 'http://www.psmsl.org/data/obtaining/rlr.annual.data/rlr_annual.zip'\n",
    "}\n",
    "\n",
    "# each station has a number of files that you can look at.\n",
    "# here we define a template for each filename\n",
    "names = {\n",
    "    'datum': '{dataset}/RLR_info/{id}.txt',\n",
    "    'diagram': '{dataset}/RLR_info/{id}.png',\n",
    "    'url': 'http://www.psmsl.org/data/obtaining/rlr.diagrams/{id}.php',\n",
    "    'data': '{dataset}/data/{id}.{typetag}data',\n",
    "    'doc': '{dataset}/docu/{id}.txt',\n",
    "    'contact': '{dataset}/docu/{id}_auth.txt',\n",
    "    'rlr_info': '{dataset}/RLR_info/{id}.txt',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function retrieves data from the NOAA Earth System Research Laboratory with which we create a dataset of the wind ad a given latitude and longitude. This data can be used for fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_wind_df(lat_i=53, lon_i=3):\n",
    "    \"\"\"\n",
    "    Create a dataset for wind, for 1 latitude/longitude\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lat_i : int\n",
    "        degree latitude\n",
    "    lon_i : int\n",
    "        degree longitude\n",
    "    \"\"\"\n",
    "    u_file = 'http://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/ncep.reanalysis.derived/surface_gauss/uwnd.10m.mon.mean.nc'\n",
    "    v_file = 'http://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/ncep.reanalysis.derived/surface_gauss/vwnd.10m.mon.mean.nc'\n",
    "\n",
    "    # open the 2 files\n",
    "    ds_u = netCDF4.Dataset(u_file)\n",
    "    ds_v = netCDF4.Dataset(v_file)\n",
    "    \n",
    "    # read lat,lon, time from 1 dataset\n",
    "    lat, lon, time = ds_u.variables['lat'][:], ds_u.variables['lon'][:], ds_u.variables['time'][:]\n",
    "    \n",
    "    # check with the others\n",
    "    lat_v, lon_v, time_v = ds_v.variables['lat'][:], ds_v.variables['lon'][:], ds_v.variables['time'][:]\n",
    "    assert (lat == lat_v).all() and (lon == lon_v).all() and (time == time_v).all()\n",
    "    \n",
    "    # convert to datetime\n",
    "    t = netCDF4.num2date(time, ds_u.variables['time'].units)\n",
    "    \n",
    "    def find_closest(lat, lon, lat_i=lat_i, lon_i=lon_i):\n",
    "        \"\"\"lookup the index of the closest lat/lon\"\"\"\n",
    "        Lon, Lat = np.meshgrid(lon, lat)\n",
    "        idx = np.argmin(((Lat - lat_i)**2 + (Lon - lon_i)**2))\n",
    "        Lat.ravel()[idx], Lon.ravel()[idx]\n",
    "        [i, j] = np.unravel_index(idx, Lat.shape)\n",
    "        return i, j\n",
    "    \n",
    "    # this is the index where we want our data\n",
    "    i, j = find_closest(lat, lon)\n",
    "    \n",
    "    # get the u, v variables\n",
    "    print('found point', lat[i], lon[j])\n",
    "    u = ds_u.variables['uwnd'][:, i, j]\n",
    "    v = ds_v.variables['vwnd'][:, i, j]\n",
    "    \n",
    "    # compute derived quantities\n",
    "    speed = np.sqrt(u ** 2 + v **2)\n",
    "    \n",
    "    # compute direction in 0-2pi domain\n",
    "    direction = np.mod(np.angle(u + v * 1j), 2*np.pi)\n",
    "    \n",
    "    # put everything in a dataframe\n",
    "    wind_df = pandas.DataFrame(data=dict(u=u, v=v, t=t, speed=speed, direction=direction))\n",
    "    wind_df = wind_df.set_index('t')\n",
    "\n",
    "    # square wind\n",
    "    wind_df['u2'] = wind_df['u']**2 * np.sign(wind_df['u'])\n",
    "    wind_df['v2'] = wind_df['v']**2 * np.sign(wind_df['v'])\n",
    "    \n",
    "    # return it\n",
    "    return wind_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the Dutch stations in the metric data, we download the overview of the stations, and select all stations with coastline code 150, which indicates a Dutch station. Another coastline_code can also be used by specifying the keyword argument coastline_code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stations(zf, dataset_name, coastline_code=150, names=None):\n",
    "    \"\"\"\n",
    "    Function to get a dataframe with the tide gauge stations within a dataset.\n",
    "    The stations are filtered on a certain coastline_code, indicating a country.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    zf : zipfile.ZipFile\n",
    "        Downloaded zipfile\n",
    "    dataset_name : string\n",
    "        Name of the dataset that is used: met_monthly, rlr_monthly, rlr_annual\n",
    "    coastline_code : int\n",
    "        Coastline code indicating the country\n",
    "    \"\"\"\n",
    "    # this list contains a table of \n",
    "    # station ID, latitude, longitude, station name, coastline code, station code, and quality flag\n",
    "    csvtext = zf.read('{}/filelist.txt'.format(dataset_name))\n",
    "    \n",
    "    # Read the stations from the comma seperated text.\n",
    "    stations = pandas.read_csv(\n",
    "        io.BytesIO(csvtext), \n",
    "        sep=';',\n",
    "        names=('id', 'lat', 'lon', 'name', 'coastline_code', 'station_code', 'quality'),\n",
    "        converters={\n",
    "            'name': str.strip,\n",
    "            'quality': str.strip\n",
    "        }\n",
    "    )\n",
    "    # Set index on column 'id'\n",
    "    stations = stations.set_index('id')\n",
    "    \n",
    "    # filter on coastline code (Netherlands is 150)\n",
    "    selected_stations = stations.where(stations['coastline_code'] == coastline_code).dropna(how='all')\n",
    "    \n",
    "    # Select on names\n",
    "    if names is not None:\n",
    "        lower_names = [name.lower() for name in names]\n",
    "        indices = [(name.lower() in lower_names) for name in stations['name'].tolist()]\n",
    "        selected_stations = stations.loc[indices]\n",
    "    \n",
    "    return selected_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_url(station, dataset):\n",
    "    \"\"\"return the url of the station information (diagram and datum)\"\"\"\n",
    "    print(dataset, station.name, dataset.split('_')[0])\n",
    "    info = dict(\n",
    "        dataset=dataset,\n",
    "        id=station.name,\n",
    "        typetag=dataset.split('_')[0]\n",
    "    )\n",
    "    url = names['url'].format(**info)\n",
    "    return url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def missing2nan(value, missing=-99999):\n",
    "    \"\"\"convert the value to nan if the float of value equals the missing value\"\"\"\n",
    "    value = float(value)\n",
    "    if value == missing:\n",
    "        return np.nan\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def year2date(year_fraction, dtype):\n",
    "    \"\"\"convert a year fraction to a datetime\"\"\"\n",
    "    startpoints = np.linspace(0, 1, num=12, endpoint=False)\n",
    "    remainder = np.mod(year_fraction, 1)\n",
    "    year = np.floor_divide(year_fraction, 1).astype('int')\n",
    "    month = np.searchsorted(startpoints, remainder)\n",
    "    if (month == 0).all():\n",
    "        # if month is set to 0 (for annual data), set to january\n",
    "        month = np.ones_like(month)\n",
    "    dates = [\n",
    "        datetime.datetime(year_i, month_i, 1) \n",
    "        for year_i, month_i \n",
    "        in zip(year, month)\n",
    "    ]\n",
    "    datetime64s = np.asarray(dates, dtype=dtype)\n",
    "    return datetime64s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rlr2nap(zf, station, dataset):\n",
    "    \"\"\"\n",
    "    Read rlr 2 nap correction from zipfile\n",
    "    \"\"\"\n",
    "    info = dict(\n",
    "        dataset=dataset,\n",
    "        id=station.name,\n",
    "    )\n",
    "    \n",
    "    bytes = zf.read(names['rlr_info'].format(**info))\n",
    "    correction = float(re.findall('Add (.+) to data .+ onwards', bytes.decode())[-1].split()[-1].replace('m', '')) * 1000\n",
    "    \n",
    "    return lambda x: x - correction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def car2nau(carthesian):\n",
    "    nautical = ((carthesian * -1) + 90) % 360\n",
    "    return nautical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(zf, wind_df, station, dataset, alpha):\n",
    "    \"\"\"\n",
    "    get data for the station (pandas record) from the dataset (url)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    zf : zipfile.ZipFile\n",
    "        Downloaded zipfile to get the data from\n",
    "    wind_df : pandas.DataFrame\n",
    "        Dataset with the wind for a given latitude and longitude\n",
    "    station : pandas.Series\n",
    "        Row of the selected_stations dataframe with station meta data\n",
    "    dataset : string\n",
    "        Name of the data set\n",
    "    alpha : dictionary\n",
    "        A dictionary with dominant wind directions for all coast stations.\n",
    "        If no wind_df is passed, alpha can be None\n",
    "    \"\"\"\n",
    "    # rlr or met\n",
    "    typetag=dataset.split('_')[0]\n",
    "    \n",
    "    info = dict(\n",
    "        dataset=dataset,\n",
    "        id=station.name,\n",
    "        typetag=typetag\n",
    "    )\n",
    "    bytes = zf.read(names['data'].format(**info))\n",
    "    converters = {\n",
    "            \"interpolated\": str.strip,\n",
    "        }\n",
    "    if typetag == 'rlr':\n",
    "        rlr2nap = get_rlr2nap(zf, station, dataset)\n",
    "        converters['height'] = lambda x: rlr2nap(missing2nan(x))\n",
    "        \n",
    "    df = pandas.read_csv(\n",
    "        io.BytesIO(bytes), \n",
    "        sep=';', \n",
    "        names=('year', 'height', 'interpolated', 'flags'),\n",
    "        converters=converters,\n",
    "    )\n",
    "    df['station'] = station.name\n",
    "    df['t'] = year2date(df.year, dtype=wind_df.index.dtype if wind_df is not None else np.datetime64)\n",
    "    df = df.set_index('t')\n",
    "    \n",
    "    if wind_df is None:\n",
    "        # Return only the water levels\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        \n",
    "#         # Calculate angles in degrees from north (clockwise)\n",
    "#         dir_car_deg = wind_df['direction'] / np.pi * 180\n",
    "#         dir_nau_deg = car2nau(dir_car_deg)\n",
    "        \n",
    "#         # Calculate the wind speed components along and perpendicular to the coastline\n",
    "#         alpha = alphas[station['name'].lower()]\n",
    "#         wind_df['u2main'] = (wind_df['speed'] * np.cos(np.pi / 180 * (dir_nau_deg - alpha)))\n",
    "#         wind_df['u2perp'] = (wind_df['speed'] * np.sin(np.pi / 180 * (dir_nau_deg - alpha)))\n",
    "        \n",
    "#         wind_df['u2main'] *= np.absolute(wind_df['u2main'])\n",
    "#         wind_df['u2perp'] *= np.absolute(wind_df['u2perp'])\n",
    "        \n",
    "        # merge the wind and water levels\n",
    "        if 'monthly' in dataset:\n",
    "            merged = pandas.merge(df, wind_df, how='left', left_index=True, right_index=True)\n",
    "        else:\n",
    "            annual_wind_df = wind_df.resample('A', label='left', loffset=datetime.timedelta(days=1)).mean()\n",
    "            merged = pandas.merge(df, annual_wind_df, how='left', left_index=True, right_index=True)\n",
    "\n",
    "        merged['u2'] = np.where(np.isnan(merged['u']), np.nanmean(merged['u2']), merged['u2'])\n",
    "        merged['v2'] = np.where(np.isnan(merged['v']), np.nanmean(merged['v2']), merged['v2'])\n",
    "        \n",
    "#         # the squared wind speed components along and perpendicular to the coastline\n",
    "#         index = np.isnan(merged['u']) | np.isnan(merged['v'])\n",
    "#         merged['u2main'] = np.where(index, np.nanmean(merged['u2main']), merged['u2main'])\n",
    "#         merged['u2perp'] = np.where(index, np.nanmean(merged['u2perp']), merged['u2perp'])\n",
    "        \n",
    "        return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function uses all functions defined above to create a dataset with the tide gauge station data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_station_data(dataset_name, coastline_code=150, names=None, include_wind=True, alphas=None):\n",
    "    \"\"\"MAIN FUNCTION. Method to get the station data for a certain dataset\"\"\"\n",
    "\n",
    "    # download the zipfile\n",
    "    resp = requests.get(urls[dataset_name])\n",
    "    \n",
    "    if include_wind:\n",
    "        wind_df = make_wind_df()\n",
    "    else:\n",
    "        wind_df = None\n",
    "      \n",
    "    # we can read the zipfile\n",
    "    stream = io.BytesIO(resp.content)\n",
    "    zf = zipfile.ZipFile(stream)\n",
    "    \n",
    "    selected_stations = get_stations(zf, dataset_name=dataset_name, coastline_code=coastline_code, names=names)\n",
    "    # fill in the dataset parameter using the global dataset_name\n",
    "    f = functools.partial(get_url, dataset=dataset_name)\n",
    "    # compute the url for each station\n",
    "    selected_stations['url'] = selected_stations.apply(f, axis=1)\n",
    "    \n",
    "    selected_stations['data'] = [get_data(zf, wind_df, station, dataset=dataset_name, alpha=alphas)\n",
    "                                 for _, station in selected_stations.iterrows()]\n",
    "   \n",
    "    return selected_stations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model for sea levels within the sea level monitor is defined in the next code block. The model is fitted on the given dataset. Wind and seasonal variability can both be taken into account when fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_model_HAC(df, with_wind=True, with_season=True):\n",
    "    \"\"\"\n",
    "    Return the fit from the linear model on the given dataset df.\n",
    "    Wind and season can be enabled and disabled\n",
    "    \"\"\"\n",
    "    y = df['height']\n",
    "    X = np.c_[\n",
    "        df['year']-1970, \n",
    "        np.cos(2*np.pi*(df['year']-1970)/18.613),\n",
    "        np.sin(2*np.pi*(df['year']-1970)/18.613)\n",
    "    ]\n",
    "    month = np.mod(df['year'], 1) * 12.0\n",
    "    names = ['Constant', 'Trend', 'Nodal U', 'Nodal V']\n",
    "    if with_wind:\n",
    "        X = np.c_[\n",
    "            X,\n",
    "            df['u2'],\n",
    "            df['v2']\n",
    "        ]\n",
    "        names.extend(['Wind U^2', 'Wind V^2'])\n",
    "    if with_season:\n",
    "        for i in range(11):\n",
    "            X = np.c_[\n",
    "                X,\n",
    "                np.logical_and(month >= i, month < i+1)\n",
    "            ]\n",
    "            names.append('month_%s' % (i+1, ))\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X, missing='drop', covtype='HAC')\n",
    "    #fit = model.fit()\n",
    "    fit = model.fit(cov_type='HAC', cov_kwds={'maxlags':1}) # 1 lag (see ACF)\n",
    "    return fit, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the statistical model\n",
    "    \"\"\"\n",
    "    Return the fit from the linear model on the given dataset df.\n",
    "    Wind and season can be enabled and disabled\n",
    "    \"\"\"\n",
    "def linear_model(df, with_wind=True, with_ar=True):\n",
    "    y = df['height']\n",
    "    X = np.c_[\n",
    "        df['year']-1970, \n",
    "        np.cos(2*np.pi*(df['year']-1970)/18.613),\n",
    "        np.sin(2*np.pi*(df['year']-1970)/18.613)\n",
    "    ]\n",
    "    month = np.mod(df['year'], 1) * 12.0\n",
    "    names = ['Constant', 'Trend', 'Nodal U', 'Nodal V']\n",
    "    if with_wind:\n",
    "        X = np.c_[\n",
    "            X, \n",
    "            df['u2'],\n",
    "            df['v2']\n",
    "        ]\n",
    "        names.extend(['Wind $u^2$', 'Wind $v^2$'])\n",
    "        \n",
    "    if with_season:\n",
    "        for i in range(11):\n",
    "            X = np.c_[\n",
    "                X,\n",
    "                np.logical_and(month >= i, month < i+1)\n",
    "            ]\n",
    "            names.append('month_%s' % (i+1, ))\n",
    "            \n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    if with_ar: \n",
    "        model = sm.GLSAR(y, X, missing='drop', rho=1) # autocorrelation of order 1\n",
    "        fit = model.fit(cov_type='HC0') # Heteroskedasticity robust standard errors\n",
    "    else:\n",
    "        model = sm.OLS(y, X, missing='drop') # no autocorrelation\n",
    "        fit = model.fit(cov_type='HAC', cov_kwds={'maxlags':1})\n",
    "        # Heteroskedasticity and autocorrelation (# 1 lag, see ACF) robust standard errors\n",
    "            \n",
    "    #fit = model.fit()\n",
    "    return fit, names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
