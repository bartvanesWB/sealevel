{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "# this is a list of packages that are used in this notebook\n",
    "# these come with python\n",
    "import io\n",
    "import zipfile\n",
    "import functools\n",
    "import bisect\n",
    "import datetime\n",
    "\n",
    "\n",
    "# you can install these packages using pip or anaconda\n",
    "# (requests numpy pandas bokeh pyproj statsmodels)\n",
    "\n",
    "# for downloading\n",
    "import requests\n",
    "import netCDF4\n",
    "\n",
    "# computation libraries\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "# statistics\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = {\n",
    "    'met_monthly': 'http://www.psmsl.org/data/obtaining/met.monthly.data/met_monthly.zip',\n",
    "    'rlr_monthly': 'http://www.psmsl.org/data/obtaining/rlr.monthly.data/rlr_monthly.zip',\n",
    "    'rlr_annual': 'http://www.psmsl.org/data/obtaining/rlr.annual.data/rlr_annual.zip'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# each station has a number of files that you can look at.\n",
    "# here we define a template for each filename\n",
    "\n",
    "# stations that we are using for our computation\n",
    "# define the name formats for the relevant files\n",
    "names = {\n",
    "    'datum': '{dataset}/RLR_info/{id}.txt',\n",
    "    'diagram': '{dataset}/RLR_info/{id}.png',\n",
    "    'url': 'http://www.psmsl.org/data/obtaining/rlr.diagrams/{id}.php',\n",
    "    'data': '{dataset}/data/{id}.{typetag}data',\n",
    "    'doc': '{dataset}/docu/{id}.txt',\n",
    "    'contact': '{dataset}/docu/{id}_auth.txt'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wind_df(lat_i=53, lon_i=3):\n",
    "    \"\"\"create a dataset for wind, for 1 latitude/longitude\"\"\"\n",
    "    u_file = 'http://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/ncep.reanalysis.derived/surface_gauss/uwnd.10m.mon.mean.nc'\n",
    "    v_file = 'http://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/ncep.reanalysis.derived/surface_gauss/vwnd.10m.mon.mean.nc'\n",
    "\n",
    "    # open the 2 files\n",
    "    ds_u = netCDF4.Dataset(u_file)\n",
    "    ds_v = netCDF4.Dataset(v_file)\n",
    "    # read lat,lon, time from 1 dataset\n",
    "    lat, lon, time = ds_u.variables['lat'][:], ds_u.variables['lon'][:], ds_u.variables['time'][:]\n",
    "    # check with the others\n",
    "    lat_v, lon_v, time_v = ds_v.variables['lat'][:], ds_v.variables['lon'][:], ds_v.variables['time'][:]\n",
    "    assert (lat == lat_v).all() and (lon == lon_v).all() and (time == time_v).all()\n",
    "    # convert to datetime\n",
    "    t = netCDF4.num2date(time, ds_u.variables['time'].units)\n",
    "    \n",
    "    def find_closest(lat, lon, lat_i=lat_i, lon_i=lon_i):\n",
    "        \"\"\"lookup the index of the closest lat/lon\"\"\"\n",
    "        Lon, Lat = np.meshgrid(lon, lat)\n",
    "        idx = np.argmin(((Lat - lat_i)**2 + (Lon - lon_i)**2))\n",
    "        Lat.ravel()[idx], Lon.ravel()[idx]\n",
    "        [i, j] = np.unravel_index(idx, Lat.shape)\n",
    "        return i, j\n",
    "    # this is the index where we want our data\n",
    "    i, j = find_closest(lat, lon)\n",
    "    # get the u, v variables\n",
    "    print('found point', lat[i], lon[j])\n",
    "    u = ds_u.variables['uwnd'][:, i, j]\n",
    "    v = ds_v.variables['vwnd'][:, i, j]\n",
    "    # compute derived quantities\n",
    "    speed = np.sqrt(u ** 2 + v **2)\n",
    "    # compute direction in 0-2pi domain\n",
    "    direction = np.mod(np.angle(u + v * 1j), 2*np.pi)\n",
    "    # put everything in a dataframe\n",
    "    wind_df = pandas.DataFrame(data=dict(u=u, v=v, t=t, speed=speed, direction=direction))\n",
    "    # return it\n",
    "    return wind_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the Dutch stations in the metric data, we download the overview of the stations, and select all stations with coastline code 150, which indicates a Dutch station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stations(zf, dataset_name, coastline_code=150):\n",
    "    # this list contains a table of \n",
    "    # station ID, latitude, longitude, station name, coastline code, station code, and quality flag\n",
    "    csvtext = zf.read('{}/filelist.txt'.format(dataset_name))\n",
    "    \n",
    "    stations = pandas.read_csv(\n",
    "        io.BytesIO(csvtext), \n",
    "        sep=';',\n",
    "        names=('id', 'lat', 'lon', 'name', 'coastline_code', 'station_code', 'quality'),\n",
    "        converters={\n",
    "            'name': str.strip,\n",
    "            'quality': str.strip\n",
    "        }\n",
    "    )\n",
    "    stations = stations.set_index('id')\n",
    "    \n",
    "    # filter on coastline code (Netherlands is 150)\n",
    "    selected_stations = stations.where(stations['coastline_code'] == coastline_code).dropna(how='all')\n",
    "    \n",
    "    return selected_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_url(station, dataset):\n",
    "    \"\"\"return the url of the station information (diagram and datum)\"\"\"\n",
    "    info = dict(\n",
    "        dataset=dataset,\n",
    "        id=station.name,\n",
    "        typetag=dataset.split('_')[0]\n",
    "    )\n",
    "    url = names['url'].format(**info)\n",
    "    return url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing2nan(value, missing=-99999):\n",
    "    \"\"\"convert the value to nan if the float of value equals the missing value\"\"\"\n",
    "    value = float(value)\n",
    "    if value == missing:\n",
    "        return np.nan\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def year2date(year_fraction, dtype):\n",
    "    startpoints = np.linspace(0, 1, num=12, endpoint=False)\n",
    "    remainder = np.mod(year_fraction, 1)\n",
    "    year = np.floor_divide(year_fraction, 1).astype('int')\n",
    "    month = np.searchsorted(startpoints, remainder)\n",
    "    dates = [\n",
    "        datetime.datetime(year_i, month_i, 1) \n",
    "        for year_i, month_i \n",
    "        in zip(year, month)\n",
    "    ]\n",
    "    datetime64s = np.asarray(dates, dtype=dtype)\n",
    "    return datetime64s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(zf, wind_df, station, dataset):\n",
    "    \"\"\"get data for the station (pandas record) from the dataset (url)\"\"\"\n",
    "    info = dict(\n",
    "        dataset=dataset,\n",
    "        id=station.name\n",
    "    )\n",
    "    bytes = zf.read(names['data'].format(**info))\n",
    "    df = pandas.read_csv(\n",
    "        io.BytesIO(bytes), \n",
    "        sep=';', \n",
    "        names=('year', 'height', 'interpolated', 'flags'),\n",
    "        converters={\n",
    "            \"interpolated\": str.strip,\n",
    "        }\n",
    "    )\n",
    "    df['station'] = station.name\n",
    "    df['t'] = year2date(df.year, dtype=wind_df.t.dtype)\n",
    "    # merge the wind and water levels\n",
    "    merged = pandas.merge(df, wind_df, how='left', on='t')\n",
    "    merged['u2'] = np.where(np.isnan(merged['u']), np.nanmean(merged['u']**2), merged['u']**2)\n",
    "    merged['v2'] = np.where(np.isnan(merged['v']), np.nanmean(merged['v']**2), merged['v']**2)\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_station_data(dataset_name, coastline_code=150):\n",
    "\n",
    "    # download the zipfile\n",
    "    resp = requests.get(urls[dataset_name])\n",
    "\n",
    "    wind_df = make_wind_df()\n",
    "      \n",
    "    # we can read the zipfile\n",
    "    stream = io.BytesIO(resp.content)\n",
    "    zf = zipfile.ZipFile(stream)\n",
    "    \n",
    "    selected_stations = get_stations(zf, dataset_name=dataset_name, coastline_code=coastline_code)\n",
    "    # fill in the dataset parameter using the global dataset_name\n",
    "    f = functools.partial(get_url, dataset=dataset_name)\n",
    "    # compute the url for each station\n",
    "    selected_stations['url'] = selected_stations.apply(f, axis=1)\n",
    "    \n",
    "    selected_stations['data'] = [get_data(zf, wind_df, station, dataset=dataset_name) for _, station in selected_stations.iterrows()]\n",
    "   \n",
    "    return selected_stations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the statistical model\n",
    "def linear_model(df, with_wind=True, with_season=True):\n",
    "    y = df['height']\n",
    "    X = np.c_[\n",
    "        df['year']-1970, \n",
    "        np.cos(2*np.pi*(df['year']-1970)/18.613),\n",
    "        np.sin(2*np.pi*(df['year']-1970)/18.613)\n",
    "    ]\n",
    "    month = np.mod(df['year'], 1) * 12.0\n",
    "    names = ['Constant', 'Trend', 'Nodal U', 'Nodal V']\n",
    "    if with_wind:\n",
    "        X = np.c_[\n",
    "            X,\n",
    "            df['u2'],\n",
    "            df['v2']\n",
    "        ]\n",
    "        names.extend(['Wind U^2', 'Wind V^2'])\n",
    "    if with_season:\n",
    "        for i in range(11):\n",
    "            X = np.c_[\n",
    "                X,\n",
    "                np.logical_and(month >= i, month < i+1)\n",
    "            ]\n",
    "            names.append('month_%s' % (i+1, ))\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X, missing='drop')\n",
    "    fit = model.fit()\n",
    "    return fit, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
